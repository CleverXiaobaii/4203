# @title ç”¨å·²è®­ç»ƒæ¨¡å‹å¤„ç†å…¨éƒ¨æ•°æ®ï¼ˆä¿®å¤æ­£åˆ™æ›¿æ¢é”™è¯¯ï¼‰
import numpy as np
import pandas as pd
import joblib
from nltk.sentiment.vader import SentimentIntensityAnalyzer
import nltk

# ä¸‹è½½å¿…è¦çš„NLTKèµ„æºï¼ˆå¦‚æœæœªä¸‹è½½ï¼‰
nltk.download('stopwords', quiet=True)
nltk.download('vader_lexicon', quiet=True)

# ===================== 1. åŠ è½½å·²è®­ç»ƒçš„æ¨¡å‹å’Œå‘é‡å™¨ =====================
print("æ­£åœ¨åŠ è½½æ¨¡å‹å’Œå‘é‡å™¨...")
try:
    lda_model = joblib.load('lda_topic_model.pkl')
    tfidf_vectorizer = joblib.load('tfidf_vectorizer.pkl')
    print("âœ… æ¨¡å‹å’Œå‘é‡å™¨åŠ è½½æˆåŠŸï¼")
except FileNotFoundError as e:
    print(f"âŒ æ‰¾ä¸åˆ°æ¨¡å‹æ–‡ä»¶ï¼š{e}")
    print("è¯·ç¡®ä¿ lda_topic_model.pkl å’Œ tfidf_vectorizer.pkl åœ¨å½“å‰ç›®å½•")
    exit()

# ===================== 2. åŠ è½½å…¨éƒ¨åŸå§‹æ•°æ® =====================
print("\næ­£åœ¨åŠ è½½å…¨éƒ¨åŸå§‹æ•°æ®...")
datafile = 'abcnews-date-text.csv'  # å®Œæ•´åŸå§‹æ•°æ®æ–‡ä»¶
try:
    # åŠ è½½å…¨éƒ¨æ•°æ®ï¼ˆä¸é™åˆ¶50Kï¼‰
    raw_data = pd.read_csv(datafile, parse_dates=[0])  # ä¿ç•™publish_dateä¸ºdatetimeç±»å‹
    print(f"âœ… åŸå§‹æ•°æ®åŠ è½½æˆåŠŸï¼Œå…± {len(raw_data):,} æ¡æ•°æ®")
except FileNotFoundError:
    print(f"âŒ æ‰¾ä¸åˆ°åŸå§‹æ•°æ®æ–‡ä»¶ï¼š{datafile}")
    print("è¯·ç¡®ä¿ abcnews-date-text.csv åœ¨å½“å‰ç›®å½•")
    exit()

# ===================== 3. æ•°æ®é¢„å¤„ç†ï¼ˆä¿®å¤æ­£åˆ™æ›¿æ¢é”™è¯¯ï¼Œç”¨pandas stræ–¹æ³•ï¼‰ =====================
print("\næ­£åœ¨é¢„å¤„ç†æ•°æ®...")

# æ–¹æ³•1ï¼šç›´æ¥ç”¨pandas stræ–¹æ³•ï¼ˆæ¨èï¼Œæ•ˆç‡é«˜ï¼Œæ”¯æŒregexï¼‰
df_full = raw_data.copy()

# 1. å°å†™è½¬æ¢ï¼ˆpandas str.lower()ï¼‰
df_full['headline_text'] = df_full['headline_text'].str.lower()

# 2. ç§»é™¤æ ‡ç‚¹ï¼ˆpandas str.replace()ï¼Œæ”¯æŒregex=Trueï¼‰
df_full['headline_text'] = df_full['headline_text'].str.replace(r'[^\w\s]', '', regex=True)

# 3. å¤„ç†å¯èƒ½çš„NaNå€¼ï¼ˆå¦‚æœæ ‡é¢˜ä¸ºç©ºï¼‰
df_full = df_full.dropna(subset=['headline_text'])
df_full = df_full[df_full['headline_text'].str.strip() != '']  # ç§»é™¤ç©ºå­—ç¬¦ä¸²æ ‡é¢˜

# ç‰¹å¾å·¥ç¨‹ï¼ˆå’Œä¹‹å‰ä¸€è‡´ï¼‰
df_full['word_count'] = df_full['headline_text'].str.split().str.len()
df_full['char_count'] = df_full['headline_text'].str.len()
df_full['year'] = df_full['publish_date'].dt.year
df_full['month'] = df_full['publish_date'].dt.month

# å»é‡
df_full = df_full.drop_duplicates(subset=['headline_text']).reset_index(drop=True)
print(f"é¢„å¤„ç†å®Œæˆï¼Œå»é‡åå‰©ä½™ {len(df_full):,} æ¡æ•°æ®")

# ===================== 4. åˆ†æ‰¹æ¬¡é¢„æµ‹ä¸»é¢˜ï¼ˆé¿å…å†…å­˜æº¢å‡ºï¼‰ =====================
print("\næ­£åœ¨é¢„æµ‹å…¨éƒ¨æ•°æ®çš„LDAä¸»é¢˜...")
# åˆ†æ‰¹æ¬¡å¤„ç†ï¼ˆ120ä¸‡æ¡æ•°æ®ä¸€æ¬¡æ€§å¤„ç†å¯èƒ½å†…å­˜ä¸è¶³ï¼Œæ¯æ‰¹1ä¸‡æ¡ï¼‰
batch_size = 10000
df_full['lda_topic'] = -1  # åˆå§‹åŒ–ä¸»é¢˜åˆ—
total_batches = len(df_full) // batch_size + 1

for batch_idx in range(total_batches):
    start = batch_idx * batch_size
    end = min((batch_idx + 1) * batch_size, len(df_full))
    batch_text = df_full.iloc[start:end]['headline_text']

    # TF-IDFå‘é‡åŒ–
    X_batch_tfidf = tfidf_vectorizer.transform(batch_text)

    # ä¸»é¢˜é¢„æµ‹
    batch_topics = lda_model.transform(X_batch_tfidf).argmax(axis=1)

    # èµ‹å€¼åˆ°åŸæ•°æ®æ¡†
    df_full.iloc[start:end, df_full.columns.get_loc('lda_topic')] = batch_topics

    # æ‰“å°è¿›åº¦
    if (batch_idx + 1) % 10 == 0 or (batch_idx + 1) == total_batches:
        print(f"å·²å®Œæˆ {batch_idx + 1}/{total_batches} æ‰¹æ¬¡ï¼ˆ{end:,}/{len(df_full):,} æ¡ï¼‰")

print("âœ… å…¨éƒ¨æ•°æ®ä¸»é¢˜é¢„æµ‹å®Œæˆ")

# ===================== 5. åˆ†æ‰¹æ¬¡è®¡ç®—æƒ…æ„Ÿåˆ†æ•° =====================
print("\næ­£åœ¨è®¡ç®—å…¨éƒ¨æ•°æ®çš„æƒ…æ„Ÿåˆ†æ•°...")
analyzer = SentimentIntensityAnalyzer()


# åˆ†æ‰¹æ¬¡è®¡ç®—æƒ…æ„Ÿåˆ†æ•°ï¼ˆé¿å…ä¸€æ¬¡æ€§å¤„ç†å‹åŠ›ï¼‰
def calculate_sentiment(text):
    scores = analyzer.polarity_scores(text)
    return pd.Series([scores['compound'], scores['pos'], scores['neg'], scores['neu']])


# åˆå§‹åŒ–æƒ…æ„Ÿåˆ—
df_full[['sentiment_compound', 'sentiment_pos', 'sentiment_neg', 'sentiment_neu']] = 0.0

for batch_idx in range(total_batches):
    start = batch_idx * batch_size
    end = min((batch_idx + 1) * batch_size, len(df_full))
    batch_text = df_full.iloc[start:end]['headline_text']

    # è®¡ç®—æƒ…æ„Ÿåˆ†æ•°
    batch_sentiments = batch_text.apply(calculate_sentiment)

    # èµ‹å€¼åˆ°åŸæ•°æ®æ¡†
    df_full.iloc[start:end, df_full.columns.get_loc('sentiment_compound')] = batch_sentiments[0].values
    df_full.iloc[start:end, df_full.columns.get_loc('sentiment_pos')] = batch_sentiments[1].values
    df_full.iloc[start:end, df_full.columns.get_loc('sentiment_neg')] = batch_sentiments[2].values
    df_full.iloc[start:end, df_full.columns.get_loc('sentiment_neu')] = batch_sentiments[3].values

    # æ‰“å°è¿›åº¦
    if (batch_idx + 1) % 10 == 0 or (batch_idx + 1) == total_batches:
        print(f"å·²å®Œæˆ {batch_idx + 1}/{total_batches} æ‰¹æ¬¡ï¼ˆ{end:,}/{len(df_full):,} æ¡ï¼‰")

# æ·»åŠ æƒ…æ„Ÿææ€§æ ‡ç­¾
df_full['sentiment_polarity'] = df_full['sentiment_compound'].apply(
    lambda x: 'æ­£å‘' if x > 0.05 else ('è´Ÿå‘' if x < -0.05 else 'ä¸­æ€§')
)
print("âœ… å…¨éƒ¨æ•°æ®æƒ…æ„Ÿåˆ†æ•°è®¡ç®—å®Œæˆ")

# ===================== 6. ä¿å­˜å¤„ç†åçš„å®Œæ•´æ•°æ® =====================
print("\næ­£åœ¨ä¿å­˜å®Œæ•´æ•°æ®...")
output_pickle = 'full_data_with_topic_sentiment.pkl'
output_csv = 'full_data_with_topic_sentiment.csv'

# ä¿å­˜ä¸ºpickleï¼ˆæ¨èï¼Œä¿ç•™æ•°æ®ç±»å‹ï¼ŒåŠ è½½æ›´å¿«ï¼‰
df_full.to_pickle(output_pickle)
# ä¿å­˜ä¸ºcsvï¼ˆå¯é€‰ï¼Œæ•°æ®é‡å¤§å¯èƒ½éœ€è¦å‡ åˆ†é’Ÿï¼‰
# df_full.to_csv(output_csv, index=False, encoding='utf-8')  # å¦‚éœ€CSVæ ¼å¼ï¼Œå–æ¶ˆæ³¨é‡Š

print(f"âœ… å®Œæ•´æ•°æ®ä¿å­˜å®Œæˆï¼")
print(f"ğŸ“ Pickleæ ¼å¼ï¼ˆæ¨èå¯è§†åŒ–ä½¿ç”¨ï¼‰ï¼š{output_pickle}")
# print(f"ğŸ“ CSVæ ¼å¼ï¼ˆæ–¹ä¾¿æŸ¥çœ‹ï¼‰ï¼š{output_csv}")  # å¦‚éœ€CSVæ ¼å¼ï¼Œå–æ¶ˆæ³¨é‡Š
print(f"ğŸ“ˆ æ•°æ®é‡ï¼š{len(df_full):,} æ¡")

# ===================== 7. éªŒè¯æ•°æ®æ ¼å¼ =====================
print("\n=== æ•°æ®æ ¼å¼éªŒè¯ ===")
required_cols = [
    'publish_date', 'headline_text', 'word_count', 'char_count', 'year', 'month',
    'sentiment_compound', 'sentiment_pos', 'sentiment_neg', 'sentiment_neu',
    'lda_topic', 'sentiment_polarity'
]
missing_cols = [col for col in required_cols if col not in df_full.columns]
if not missing_cols:
    print("âœ… æ‰€æœ‰å¿…è¦åˆ—éƒ½å­˜åœ¨ï¼Œå¯ç›´æ¥å¯¹æ¥å¯è§†åŒ–è„šæœ¬ï¼")
else:
    print(f"âš ï¸  ç¼ºå°‘åˆ—ï¼š{missing_cols}")

# è¾“å‡ºä¸»é¢˜åˆ†å¸ƒé¢„è§ˆ
print(f"\n=== å…¨éƒ¨æ•°æ®ä¸»é¢˜åˆ†å¸ƒé¢„è§ˆ ===")
topic_dist = df_full['lda_topic'].value_counts().sort_index()
for topic_id, count in topic_dist.items():
    print(f"ä¸»é¢˜ {topic_id}: {count:,} æ¡ ({count / len(df_full) * 100:.1f}%)")
