# ===================== æ”¹è¿›ç‰ˆæœ¬ï¼šä½¿ç”¨è¯­ä¹‰ç›¸ä¼¼åº¦ + æœºå™¨å­¦ä¹ åˆ†ç±» =====================
# ç‰ˆæœ¬ï¼šv2.2 - æ·»åŠ è‡ªåŠ¨æ•°æ®é‡‡æ ·ï¼ˆ1/5ï¼‰ï¼Œå¿«é€Ÿæµ‹è¯•ä¸“ç”¨ç‰ˆæœ¬
import numpy as np
import pandas as pd
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report
from sklearn.preprocessing import StandardScaler
import matplotlib.pyplot as plt
import seaborn as sns
from scipy.stats import pearsonr
import re
import time

# è¿›åº¦æ¡ç±»ï¼ˆç¾åŒ–è¾“å‡ºï¼‰
class ProgressBar:
    def __init__(self, total, desc="Processing"):
        self.total = total
        self.desc = desc
        self.current = 0
        self.start_time = time.time()
    
    def update(self, n=1):
        self.current += n
        percent = self.current / self.total
        elapsed = time.time() - self.start_time
        rate = self.current / elapsed if elapsed > 0 else 0
        remaining = (self.total - self.current) / rate if rate > 0 else 0
        
        bar_length = 50
        filled = int(bar_length * percent)
        bar = 'â–ˆ' * filled + 'â–‘' * (bar_length - filled)
        
        print(f"\r[{self.desc}] {bar} {percent*100:5.1f}% ({self.current}/{self.total}) | "
              f"Elapsed: {int(elapsed)}s | Remaining: {int(remaining)}s", end='', flush=True)
    
    def finish(self):
        print()  # æ¢è¡Œ


def improved_fake_news_detection_ml_based(
    sensationalism_pkl='full_data_with_sensationalism_score.pkl',
    fake_news_csv='Fake.csv',
    real_news_csv='True.csv',
    threshold=65,
    sample_size=None,  # å¯é€‰ï¼šåªå¤„ç†å‰Næ¡æ•°æ®ï¼Œç”¨äºå¿«é€Ÿæµ‹è¯•
    sampling_ratio=0.2,  # ã€æ–°å¢ã€‘ä»CSVä¸­é‡‡æ ·çš„æ¯”ä¾‹ï¼ˆ0.2 = 1/5ï¼‰
    main_data_sampling_ratio=0.2  # ã€æ–°å¢ã€‘ä¸»æ•°æ®é›†é‡‡æ ·æ¯”ä¾‹ï¼ˆé»˜è®¤1/10ï¼‰
):
    """
    æ”¹è¿›ç‰ˆå‡æ–°é—»æ£€æµ‹ï¼ˆå¸¦è¯¦ç»†è¿›åº¦æç¤ºï¼‰ï¼š
    æ ¸å¿ƒæ€è·¯ï¼šä¸ç›´æ¥ç”¨ç›¸ä¼¼åº¦åˆ¤æ–­ï¼Œè€Œæ˜¯æ„å»ºç‰¹å¾å‘é‡ â†’ è®­ç»ƒæœºå™¨å­¦ä¹ æ¨¡å‹
    è¿™æ ·èƒ½åˆ©ç”¨å¤šç‰¹å¾ç»„åˆï¼Œè€Œä¸æ˜¯ä¾èµ–å•ä¸€çš„ç›¸ä¼¼åº¦æŒ‡æ ‡
    
    ã€v2.2ç‰ˆæœ¬æ–°å¢åŠŸèƒ½ã€‘
    - è‡ªåŠ¨ä»Fake.csvå’ŒTrue.csvé‡‡æ ·ï¼ˆé»˜è®¤1/5æ•°æ®ï¼‰
    - ä½¿é€Ÿåº¦å¿«5å€
    - ä¿æŒç»“æœå‡†ç¡®æ€§
    - æœ€ä½³ç”¨äºå¿«é€Ÿæµ‹è¯•
    
    å‚æ•°ï¼š
        sampling_ratio: é‡‡æ ·æ¯”ä¾‹ï¼Œé»˜è®¤0.2ï¼ˆ1/5ï¼‰
                       å¯æ”¹ä¸º 0.5ï¼ˆ1/2ï¼‰ã€1.0ï¼ˆå…¨éƒ¨ï¼‰ç­‰
    """
    from rapidfuzz import process, fuzz
    
    print("\n" + "="*80)
    print("="*80)
    print("        ğŸš€ æ”¹è¿›ç‰ˆå‡æ–°é—»æ£€æµ‹ç³»ç»Ÿï¼ˆåŸºäºæœºå™¨å­¦ä¹ åˆ†ç±»ï¼‰v2.2")
    print("        ğŸ”¥ å¿«é€Ÿæµ‹è¯•ç‰ˆæœ¬ - è‡ªåŠ¨é‡‡æ ·1/5æ•°æ®ï¼Œè¿è¡Œé€Ÿåº¦æå‡5å€")
    print("="*80)
    print("="*80)
    
    # ==================== STEP 1: åŠ è½½åŸºç¡€æ•°æ® ====================
    print("\n" + "â–¶"*40)
    print("STEP 1/6: åŠ è½½åŸºç¡€æ•°æ®")
    print("â–¶"*40)
    
    try:
        print("[1/3] æ­£åœ¨åŠ è½½ä¸»æ•°æ®é›†ï¼ˆå¸¦è€¸äººæŒ‡æ•°ï¼‰...")
        df_sens = pd.read_pickle(sensationalism_pkl).sample(frac=main_data_sampling_ratio, random_state=42).reset_index(drop=True)
        if sample_size:
            df_sens = df_sens.iloc[:sample_size].copy()
            print(f"    âš ï¸  é‡‡æ ·æ¨¡å¼ï¼šä»…å¤„ç†å‰ {sample_size} æ¡æ•°æ®")
        print(f"    âœ… æˆåŠŸåŠ è½½ï¼š{len(df_sens):,} æ¡æ•°æ®")
        print(f"    ğŸ“Š æ•°æ®åˆ—åï¼š{df_sens.columns.tolist()}")
    except Exception as e:
        print(f"    âŒ æ•°æ®åŠ è½½å¤±è´¥ï¼š{e}")
        return
    
    # ==================== STEP 2: åŠ è½½å’Œå¤„ç†æ ‡æ³¨æ•°æ®ï¼ˆå¸¦é‡‡æ ·ï¼‰ ====================
    print("\n[2/3] æ­£åœ¨åŠ è½½æ ‡æ³¨æ•°æ®ï¼ˆå‡æ–°é—» + çœŸå®æ–°é—»ï¼‰...")
    
    def clean_text(text):
        """æ¸…æ´—æ–‡æœ¬"""
        text = str(text).strip().lower()
        text = re.sub(r'[^\w\s]', '', text)
        text = re.sub(r'\s+', ' ', text)
        return text
    
    try:
        # åŠ è½½å‡æ–°é—»ï¼ˆå¸¦é‡‡æ ·ï¼‰
        print("    [2.1/3] åŠ è½½å‡æ–°é—»æ•°æ®...")
        fake_df = pd.read_csv(fake_news_csv)
        original_fake_size = len(fake_df)
        fake_df = fake_df.sample(frac=sampling_ratio, random_state=42).reset_index(drop=True)
        print(f"        âœ… åŠ è½½å‡æ–°é—» {len(fake_df):,} æ¡ / åŸå§‹ {original_fake_size:,} æ¡ (é‡‡æ ·æ¯”ä¾‹ {sampling_ratio*100:.0f}%)")
        
        title_col_fake = [col for col in fake_df.columns if col.lower().strip() == 'title']
        if not title_col_fake:
            raise ValueError(f"å‡æ–°é—»CSVç¼ºå°‘'title'åˆ—ã€‚å½“å‰åˆ—ï¼š{fake_df.columns.tolist()}")
        fake_df = fake_df.rename(columns={title_col_fake[0]: 'headline'})
        fake_df['is_fake'] = 1
        
        # åŠ è½½çœŸå®æ–°é—»ï¼ˆå¸¦é‡‡æ ·ï¼‰
        print("    [2.2/3] åŠ è½½çœŸå®æ–°é—»æ•°æ®...")
        real_df = pd.read_csv(real_news_csv)
        original_real_size = len(real_df)
        real_df = real_df.sample(frac=sampling_ratio, random_state=42).reset_index(drop=True)
        print(f"        âœ… åŠ è½½çœŸå®æ–°é—» {len(real_df):,} æ¡ / åŸå§‹ {original_real_size:,} æ¡ (é‡‡æ ·æ¯”ä¾‹ {sampling_ratio*100:.0f}%)")
        
        title_col_real = [col for col in real_df.columns if col.lower().strip() == 'title']
        if not title_col_real:
            raise ValueError(f"çœŸå®æ–°é—»CSVç¼ºå°‘'title'åˆ—ã€‚å½“å‰åˆ—ï¼š{real_df.columns.tolist()}")
        real_df = real_df.rename(columns={title_col_real[0]: 'headline'})
        real_df['is_fake'] = 0
        
        # åˆå¹¶
        labeled_df = pd.concat([fake_df[['headline', 'is_fake']], 
                                real_df[['headline', 'is_fake']]], ignore_index=True)
        
        # æ¸…æ´—å’Œå»é‡
        print("    [2.3/3] æ¸…æ´—å’Œå»é‡...")
        labeled_df['headline_clean'] = labeled_df['headline'].apply(clean_text)
        labeled_df = labeled_df[labeled_df['headline_clean'] != ""].drop_duplicates(subset=['headline_clean'])
        
        # ã€å…³é”®ä¿®å¤ã€‘é‡ç½®ç´¢å¼•ï¼Œç¡®ä¿ä¸åç»­åˆ—è¡¨ç´¢å¼•å¯¹é½
        labeled_df = labeled_df.reset_index(drop=True)
        
        print(f"        âœ… æ ‡æ³¨æ•°æ®åˆå¹¶å®Œæˆï¼š{len(labeled_df):,} æ¡")
        print(f"           - å‡æ–°é—»ï¼š{labeled_df['is_fake'].sum():,} æ¡")
        print(f"           - çœŸå®æ–°é—»ï¼š{(1-labeled_df['is_fake']).sum():,} æ¡")
        print(f"           - å‡æ–°é—»æ¯”ä¾‹ï¼š{labeled_df['is_fake'].mean()*100:.1f}%")
        
    except Exception as e:
        print(f"    âŒ æ ‡æ³¨æ•°æ®åŠ è½½å¤±è´¥ï¼š{e}")
        return
    
    # ==================== STEP 2: æ¸…æ´—ä¸»æ•°æ®é›† ====================
    print("\n" + "â–¶"*40)
    print("STEP 2/6: æ¸…æ´—ä¸»æ•°æ®é›†")
    print("â–¶"*40)
    
    print("[1/1] æ¸…æ´—ä¸»æ•°æ®é›†...")
    df_sens['headline_clean'] = df_sens['headline_text'].apply(clean_text)
    df_sens = df_sens[df_sens['headline_clean'] != ""].drop_duplicates(subset=['headline_clean'])
    df_sens = df_sens.reset_index(drop=True)  # é‡ç½®ç´¢å¼•
    print(f"    âœ… ä¸»æ•°æ®é›†æ¸…æ´—å®Œæˆï¼š{len(df_sens):,} æ¡æœ‰æ•ˆæ•°æ®")
    
    # ==================== STEP 3: ç‰¹å¾æå– ====================
    print("\n" + "â–¶"*40)
    print("STEP 3/6: æå–å¤šç»´ç‰¹å¾")
    print("â–¶"*40)
    
    print("ç‰¹å¾ç»´åº¦è¯´æ˜ï¼š")
    print("  1ï¸âƒ£  max_similarity - ä¸æ ‡æ³¨æ•°æ®çš„æœ€å¤§ç›¸ä¼¼åº¦ï¼ˆ0-1ï¼‰")
    print("  2ï¸âƒ£  sensationalism_score - è€¸äººå¬é—»æŒ‡æ•°ï¼ˆå·²æœ‰ï¼Œ0-1ï¼‰")
    print("  3ï¸âƒ£  headline_length_norm - æ ‡é¢˜é•¿åº¦å½’ä¸€åŒ–ï¼ˆ0-1ï¼‰")
    print("  4ï¸âƒ£  sentiment_extremity - æƒ…æ„Ÿæç«¯ç¨‹åº¦ï¼ˆ0-1ï¼‰")
    print("  5ï¸âƒ£  negative_bias - æ˜¯å¦ä¸ºè´Ÿé¢æ–°é—»ï¼ˆ0 or 1ï¼‰")
    
    # ç‰¹å¾4.1ï¼šæœ€é«˜ç›¸ä¼¼åº¦ï¼ˆè¯­ä¹‰ç›¸ä¼¼æ€§ï¼‰
    print("\n[1/5] è®¡ç®—ç›¸ä¼¼åº¦ç‰¹å¾...")
    labeled_texts = labeled_df['headline_clean'].tolist()
    
    def get_max_similarity_batch(texts, labeled_texts, batch_size=500):
        """æ‰¹é‡è®¡ç®—ç›¸ä¼¼åº¦ï¼Œå¸¦è¿›åº¦æç¤º"""
        similarities = []
        progress = ProgressBar(len(texts), desc="Similarity")
        
        for i in range(0, len(texts), batch_size):
            batch = texts[i:i+batch_size]
            for text in batch:
                if not text:
                    similarities.append(0)
                else:
                    try:
                        matches = process.extract(text, labeled_texts, 
                                                 scorer=fuzz.token_sort_ratio, limit=1)
                        sim = matches[0][1] / 100 if matches else 0
                    except:
                        sim = 0
                    similarities.append(sim)
                progress.update(1)
        
        progress.finish()
        return similarities
    
    df_sens['max_similarity'] = get_max_similarity_batch(
        df_sens['headline_clean'].tolist(), 
        labeled_texts,
        batch_size=500
    )
    print(f"    âœ… å®Œæˆ | ç›¸ä¼¼åº¦èŒƒå›´ï¼š{df_sens['max_similarity'].min():.3f} - {df_sens['max_similarity'].max():.3f}")
    
    # ç‰¹å¾4.2ï¼šå·²æœ‰çš„è€¸äººå¬é—»åˆ†æ•°ï¼ˆç›´æ¥å¤ç”¨ï¼‰
    print("[2/5] éªŒè¯è€¸äººå¬é—»ç‰¹å¾...")
    if 'sensationalism_score' not in df_sens.columns:
        print("    âš ï¸  è­¦å‘Šï¼šæ•°æ®ä¸­ç¼ºå°‘sensationalism_scoreåˆ—")
        print("    ä½¿ç”¨é»˜è®¤å€¼ 0.0")
        df_sens['sensationalism_score'] = 0.0
    else:
        print(f"    âœ… ç‰¹å¾å·²å­˜åœ¨ | èŒƒå›´ï¼š{df_sens['sensationalism_score'].min():.3f} - {df_sens['sensationalism_score'].max():.3f}")
    
    # ç‰¹å¾4.3ï¼šé•¿åº¦ç‰¹å¾
    print("[3/5] è®¡ç®—æ ‡é¢˜é•¿åº¦ç‰¹å¾...")
    progress = ProgressBar(len(df_sens), desc="Length")
    df_sens['headline_length'] = df_sens['headline_text'].apply(
        lambda x: len(str(x).split())
    )
    progress.update(len(df_sens))
    progress.finish()
    
    df_sens['headline_length_norm'] = (df_sens['headline_length'] - df_sens['headline_length'].min()) / \
                                       (df_sens['headline_length'].max() - df_sens['headline_length'].min() + 1e-8)
    print(f"    âœ… å®Œæˆ | é•¿åº¦èŒƒå›´ï¼š{df_sens['headline_length'].min()} - {df_sens['headline_length'].max()} è¯")
    
    # ç‰¹å¾4.4ï¼šæƒ…æ„Ÿæç«¯æ€§
    print("[4/5] è®¡ç®—æƒ…æ„Ÿæç«¯æ€§ç‰¹å¾...")
    progress = ProgressBar(len(df_sens), desc="Sentiment")
    df_sens['sentiment_extremity'] = df_sens['sentiment_compound'].apply(lambda x: abs(x))
    progress.update(len(df_sens))
    progress.finish()
    print(f"    âœ… å®Œæˆ | æç«¯æ€§èŒƒå›´ï¼š{df_sens['sentiment_extremity'].min():.3f} - {df_sens['sentiment_extremity'].max():.3f}")
    
    # ç‰¹å¾4.5ï¼šè´Ÿé¢å€¾å‘
    print("[5/5] è®¡ç®—è´Ÿé¢å€¾å‘ç‰¹å¾...")
    progress = ProgressBar(len(df_sens), desc="Negative")
    df_sens['negative_bias'] = df_sens['sentiment_compound'].apply(lambda x: 1 if x < -0.1 else 0)
    progress.update(len(df_sens))
    progress.finish()
    print(f"    âœ… å®Œæˆ | è´Ÿé¢æ¯”ä¾‹ï¼š{df_sens['negative_bias'].mean()*100:.1f}%")
    
    # ==================== STEP 4: æ„å»ºè®­ç»ƒæ•°æ® ====================
    print("\n" + "â–¶"*40)
    print("STEP 4/6: æ„å»ºè®­ç»ƒé›†ï¼ˆä»æ ‡æ³¨æ•°æ®ï¼‰")
    print("â–¶"*40)
    
    print("[1/3] è®¡ç®—æ ‡æ³¨æ•°æ®çš„ç›¸ä¼¼åº¦ç‰¹å¾...")
    training_texts = labeled_df['headline_clean'].tolist()
    training_labels = labeled_df['is_fake'].tolist()
    
    training_max_sim = []
    progress = ProgressBar(len(training_texts), desc="Sim")
    
    for i, text in enumerate(training_texts):
        other_texts = [t for j, t in enumerate(training_texts) if j != i]
        if other_texts:
            try:
                matches = process.extract(text, other_texts, 
                                         scorer=fuzz.token_sort_ratio, limit=1)
                sim = matches[0][1] / 100 if matches else 0
            except:
                sim = 0
        else:
            sim = 0
        training_max_sim.append(sim)
        progress.update(1)
    
    progress.finish()
    
    # ã€å…³é”®æ£€æŸ¥ã€‘ç¡®ä¿åˆ—è¡¨é•¿åº¦ä¸€è‡´
    if len(training_max_sim) != len(labeled_df):
        print(f"    âš ï¸  è­¦å‘Šï¼šé•¿åº¦ä¸åŒ¹é… | training_max_simé•¿åº¦={len(training_max_sim)} != labeled_dfé•¿åº¦={len(labeled_df)}")
        print(f"    æ­£åœ¨ä¿®å¤...")
        # å¦‚æœé•¿åº¦ä¸ä¸€è‡´ï¼Œæˆªæ–­æˆ–è¡¥å……
        if len(training_max_sim) > len(labeled_df):
            training_max_sim = training_max_sim[:len(labeled_df)]
        else:
            training_max_sim.extend([0] * (len(labeled_df) - len(training_max_sim)))
    
    print("[2/3] ä¸ºæ ‡æ³¨æ•°æ®åˆ†é…å…¶ä»–ç‰¹å¾...")
    # ã€æ”¹è¿›ã€‘ç›´æ¥ä½¿ç”¨indexed approachè€Œä¸æ˜¯iterrowsï¼ˆæ›´å¿«ä¸”é¿å…ç´¢å¼•é—®é¢˜ï¼‰
    training_data_list = []
    progress = ProgressBar(len(labeled_df), desc="Features")
    
    for idx in range(len(labeled_df)):
        # ç›´æ¥ç”¨ç´¢å¼•è®¿é—®
        row = labeled_df.iloc[idx]
        
        # åœ¨ä¸»æ•°æ®é›†ä¸­æŸ¥æ‰¾ç›¸åŒæ–‡æœ¬
        matching_rows = df_sens[df_sens['headline_clean'] == row['headline_clean']]
        
        if len(matching_rows) > 0:
            # å¦‚æœåœ¨ä¸»æ•°æ®é›†ä¸­æ‰¾åˆ°ï¼Œç›´æ¥å¤ç”¨ç‰¹å¾
            feat_row = matching_rows.iloc[0]
            training_data_list.append({
                'max_similarity': training_max_sim[idx],
                'sensationalism_score': feat_row.get('sensationalism_score', 0),
                'headline_length_norm': feat_row.get('headline_length_norm', 0),
                'sentiment_extremity': feat_row.get('sentiment_extremity', 0),
                'negative_bias': feat_row.get('negative_bias', 0),
                'is_fake': row['is_fake']
            })
        else:
            # å¦åˆ™è®¡ç®—ç‰¹å¾
            text_len = len(row['headline'].split())
            text_len_norm = (text_len - df_sens['headline_length'].min()) / \
                           (df_sens['headline_length'].max() - df_sens['headline_length'].min() + 1e-8)
            training_data_list.append({
                'max_similarity': training_max_sim[idx],
                'sensationalism_score': 0.5,  # é»˜è®¤ä¸­ç­‰
                'headline_length_norm': text_len_norm,
                'sentiment_extremity': 0.5,
                'negative_bias': 0,
                'is_fake': row['is_fake']
            })
        
        progress.update(1)
    
    progress.finish()
    
    print("[3/3] æ•´ç†è®­ç»ƒé›†...")
    training_df = pd.DataFrame(training_data_list)
    training_df = training_df.dropna()
    print(f"    âœ… è®­ç»ƒé›†æ„å»ºå®Œæˆï¼š{len(training_df)} æ¡")
    print(f"       - å‡æ–°é—»æ ·æœ¬ï¼š{training_df['is_fake'].sum()} æ¡ï¼ˆ{training_df['is_fake'].mean()*100:.1f}%ï¼‰")
    print(f"       - çœŸå®æ–°é—»æ ·æœ¬ï¼š{len(training_df) - training_df['is_fake'].sum()} æ¡")
    
    # ==================== STEP 5: æ¨¡å‹è®­ç»ƒ ====================
    print("\n" + "â–¶"*40)
    print("STEP 5/6: è®­ç»ƒæœºå™¨å­¦ä¹ æ¨¡å‹")
    print("â–¶"*40)
    
    print("[1/4] æ ‡å‡†åŒ–ç‰¹å¾...")
    feature_cols = ['max_similarity', 'sensationalism_score', 'headline_length_norm', 
                    'sentiment_extremity', 'negative_bias']
    
    scaler = StandardScaler()
    X_train = scaler.fit_transform(training_df[feature_cols])
    y_train = training_df['is_fake'].values
    print(f"    âœ… ç‰¹å¾æ ‡å‡†åŒ–å®Œæˆ | ç‰¹å¾ç»´åº¦ï¼š{X_train.shape}")
    
    # æ¨¡å‹1ï¼šéšæœºæ£®æ—
    print("\n[2/4] è®­ç»ƒéšæœºæ£®æ—æ¨¡å‹...")
    print("    â³ è¿™å¯èƒ½éœ€è¦ 10-30 ç§’ï¼ˆå–å†³äºé‡‡æ ·æ•°æ®é‡ï¼‰...")
    
    rf_model = RandomForestClassifier(
        n_estimators=100,
        max_depth=10,
        random_state=42,
        n_jobs=-1,
        verbose=0
    )
    
    start_time = time.time()
    rf_model.fit(X_train, y_train)
    train_time_rf = time.time() - start_time
    
    rf_train_acc = accuracy_score(y_train, rf_model.predict(X_train))
    rf_train_prec = precision_score(y_train, rf_model.predict(X_train), zero_division=0)
    rf_train_rec = recall_score(y_train, rf_model.predict(X_train), zero_division=0)
    rf_train_f1 = f1_score(y_train, rf_model.predict(X_train), zero_division=0)
    
    print(f"    âœ… è®­ç»ƒå®Œæˆï¼ˆè€—æ—¶ {train_time_rf:.1f}sï¼‰")
    print(f"       ğŸ“Š è®­ç»ƒé›†æ€§èƒ½ï¼šå‡†ç¡®ç‡ {rf_train_acc:.2%} | ç²¾åº¦ {rf_train_prec:.2%} | å¬å› {rf_train_rec:.2%} | F1 {rf_train_f1:.2%}")
    
    # æ¨¡å‹2ï¼šé€»è¾‘å›å½’
    print("\n[3/4] è®­ç»ƒé€»è¾‘å›å½’æ¨¡å‹...")
    print("    â³ è¿™é€šå¸¸éœ€è¦ 2-5 ç§’...")
    
    lr_model = LogisticRegression(random_state=42, max_iter=200, verbose=0)
    
    start_time = time.time()
    lr_model.fit(X_train, y_train)
    train_time_lr = time.time() - start_time
    
    lr_train_acc = accuracy_score(y_train, lr_model.predict(X_train))
    lr_train_prec = precision_score(y_train, lr_model.predict(X_train), zero_division=0)
    lr_train_rec = recall_score(y_train, lr_model.predict(X_train), zero_division=0)
    lr_train_f1 = f1_score(y_train, lr_model.predict(X_train), zero_division=0)
    
    print(f"    âœ… è®­ç»ƒå®Œæˆï¼ˆè€—æ—¶ {train_time_lr:.1f}sï¼‰")
    print(f"       ğŸ“Š è®­ç»ƒé›†æ€§èƒ½ï¼šå‡†ç¡®ç‡ {lr_train_acc:.2%} | ç²¾åº¦ {lr_train_prec:.2%} | å¬å› {lr_train_rec:.2%} | F1 {lr_train_f1:.2%}")
    
    print("\n[4/4] æ¨¡å‹å¯¹æ¯”...")
    print("    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”")
    print("    â”‚ æ¨¡å‹            â”‚ å‡†ç¡®ç‡(%)  â”‚ ç²¾åº¦(%)    â”‚ å¬å›(%)    â”‚ F1-Score   â”‚")
    print("    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤")
    print(f"    â”‚ éšæœºæ£®æ—        â”‚ {rf_train_acc*100:6.2f}     â”‚ {rf_train_prec*100:6.2f}     â”‚ {rf_train_rec*100:6.2f}     â”‚ {rf_train_f1:.4f}     â”‚")
    print(f"    â”‚ é€»è¾‘å›å½’        â”‚ {lr_train_acc*100:6.2f}     â”‚ {lr_train_prec*100:6.2f}     â”‚ {lr_train_rec*100:6.2f}     â”‚ {lr_train_f1:.4f}     â”‚")
    print("    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜")
    print(f"    ğŸ’¡ æ¨èæ¨¡å‹ï¼š{'éšæœºæ£®æ—' if rf_train_f1 > lr_train_f1 else 'é€»è¾‘å›å½’'}ï¼ˆF1åˆ†æ•°æ›´é«˜ï¼‰")
    
    # ==================== STEP 6: åœ¨ä¸»æ•°æ®é›†ä¸Šé¢„æµ‹ ====================
    print("\n" + "â–¶"*40)
    print("STEP 6/6: å¯¹ä¸»æ•°æ®é›†è¿›è¡Œé¢„æµ‹")
    print("â–¶"*40)
    
    print("[1/3] å‡†å¤‡é¢„æµ‹æ•°æ®...")
    df_pred = df_sens.copy()
    df_pred = df_pred.dropna(subset=feature_cols)
    print(f"    âœ… å‡†å¤‡å®Œæˆï¼š{len(df_pred)} æ¡æœ‰æ•ˆæ•°æ®ï¼ˆç¼ºå¤±ç‰¹å¾å·²å»é™¤ï¼‰")
    
    print("\n[2/3] ä½¿ç”¨éšæœºæ£®æ—æ¨¡å‹é¢„æµ‹...")
    progress = ProgressBar(len(df_pred), desc="RF Predict")
    
    X_pred = scaler.transform(df_pred[feature_cols])
    
    # åˆ†æ‰¹é¢„æµ‹ï¼ˆæ˜¾ç¤ºè¿›åº¦ï¼‰
    batch_size = 5000
    rf_preds = []
    rf_probs = []
    
    for i in range(0, len(X_pred), batch_size):
        batch_X = X_pred[i:i+batch_size]
        rf_preds.extend(rf_model.predict(batch_X))
        rf_probs.extend(rf_model.predict_proba(batch_X)[:, 1])
        progress.update(len(batch_X))
    
    progress.finish()
    
    df_pred['fake_pred_rf'] = rf_preds
    df_pred['fake_prob_rf'] = rf_probs
    
    print(f"    âœ… é¢„æµ‹å®Œæˆ")
    print(f"       - é¢„æµ‹å‡æ–°é—»æ¯”ä¾‹ï¼š{df_pred['fake_pred_rf'].mean()*100:.1f}%")
    print(f"       - æ¦‚ç‡èŒƒå›´ï¼š{df_pred['fake_prob_rf'].min():.3f} - {df_pred['fake_prob_rf'].max():.3f}")
    
    print("\n[3/3] ä½¿ç”¨é€»è¾‘å›å½’æ¨¡å‹é¢„æµ‹...")
    progress = ProgressBar(len(df_pred), desc="LR Predict")
    
    lr_preds = []
    lr_probs = []
    
    for i in range(0, len(X_pred), batch_size):
        batch_X = X_pred[i:i+batch_size]
        lr_preds.extend(lr_model.predict(batch_X))
        lr_probs.extend(lr_model.predict_proba(batch_X)[:, 1])
        progress.update(len(batch_X))
    
    progress.finish()
    
    df_pred['fake_pred_lr'] = lr_preds
    df_pred['fake_prob_lr'] = lr_probs
    
    print(f"    âœ… é¢„æµ‹å®Œæˆ")
    print(f"       - é¢„æµ‹å‡æ–°é—»æ¯”ä¾‹ï¼š{df_pred['fake_pred_lr'].mean()*100:.1f}%")
    print(f"       - æ¦‚ç‡èŒƒå›´ï¼š{df_pred['fake_prob_lr'].min():.3f} - {df_pred['fake_prob_lr'].max():.3f}")


    # ==================== ä¸»é¢˜çº§åˆ†æ ====================
    print("\n" + "â–¶"*40)
    print("ä¸»é¢˜çº§åˆ†æä¸ç›¸å…³æ€§è®¡ç®—")
    print("â–¶"*40)
    
    print("[1/3] è®¡ç®—ä¸»é¢˜çº§æŒ‡æ ‡...")
    progress = ProgressBar(df_pred['lda_topic'].nunique(), desc="Topics")
    
    topic_analysis = df_pred.groupby('lda_topic').agg(
        headline_count=('headline_text', 'count'),
        predicted_fake_ratio_rf=('fake_pred_rf', 'mean'),
        predicted_fake_ratio_lr=('fake_pred_lr', 'mean'),
        avg_fake_prob_rf=('fake_prob_rf', 'mean'),
        avg_fake_prob_lr=('fake_prob_lr', 'mean'),
        avg_sensationalism=('sensationalism_score', 'mean'),
        avg_sentiment=('sentiment_compound', 'mean'),
        avg_similarity=('max_similarity', 'mean')
    ).reset_index()
    
    topic_analysis = topic_analysis[topic_analysis['headline_count'] >= 20].reset_index(drop=True)
    progress.update(len(topic_analysis))
    progress.finish()
    
    print(f"    âœ… å®Œæˆ | å…± {len(topic_analysis)} ä¸ªæœ‰æ•ˆä¸»é¢˜ï¼ˆâ‰¥20æ¡æ•°æ®ï¼‰")
    
    print("\n[2/3] è®¡ç®—ç›¸å…³æ€§ç³»æ•°...")
    
    # ç›¸å…³æ€§åˆ†æ (RF)
    corr_rf, p_rf = pearsonr(topic_analysis['avg_sensationalism'], 
                             topic_analysis['predicted_fake_ratio_rf'])
    
    # ç›¸å…³æ€§åˆ†æ (LR)
    corr_lr, p_lr = pearsonr(topic_analysis['avg_sensationalism'], 
                             topic_analysis['predicted_fake_ratio_lr'])
    
    print(f"    âœ… å®Œæˆ")
    print(f"\n    ğŸ“Š ç›¸å…³æ€§åˆ†æç»“æœï¼ˆçš®å°”é€Šç›¸å…³ç³»æ•°ï¼‰ï¼š")
    print(f"    â”œâ”€ éšæœºæ£®æ—æ¨¡å‹ï¼š")
    print(f"    â”‚  â”œâ”€ ç›¸å…³ç³»æ•° r = {corr_rf:+.4f}")
    print(f"    â”‚  â”œâ”€ p å€¼ = {p_rf:.4f}")
    print(f"    â”‚  â””â”€ ç»“æœï¼š{'âœ… æ˜¾è‘—ç›¸å…³ (p<0.05)' if p_rf < 0.05 else 'âŒ ä¸æ˜¾è‘— (pâ‰¥0.05)'}")
    print(f"    â”œâ”€ é€»è¾‘å›å½’æ¨¡å‹ï¼š")
    print(f"    â”‚  â”œâ”€ ç›¸å…³ç³»æ•° r = {corr_lr:+.4f}")
    print(f"    â”‚  â”œâ”€ p å€¼ = {p_lr:.4f}")
    print(f"    â”‚  â””â”€ ç»“æœï¼š{'âœ… æ˜¾è‘—ç›¸å…³ (p<0.05)' if p_lr < 0.05 else 'âŒ ä¸æ˜¾è‘— (pâ‰¥0.05)'}")
    print(f"    â””â”€ è§£è¯»ï¼š")
    if abs(corr_rf) > 0.5 or abs(corr_lr) > 0.5:
        print(f"       ğŸ¯ å¼ºç›¸å…³æ€§ï¼è€¸äººå¬é—»æŒ‡æ•°ä¸å‡æ–°é—»å­˜åœ¨æ˜¾è‘—å…³è”")
    elif abs(corr_rf) > 0.3 or abs(corr_lr) > 0.3:
        print(f"       âš ï¸  ä¸­ç­‰ç›¸å…³æ€§ï¼Œå­˜åœ¨ä¸€å®šå…³è”ä½†ä¸å¤Ÿå¼º")
    else:
        print(f"       ğŸ“Š å¼±ç›¸å…³æ€§ï¼Œå¯èƒ½éœ€è¦å¢åŠ æ›´å¤šç‰¹å¾")
    
    print("\n[3/3] ä¸»é¢˜è¯¦ç»†æ•°æ®...")
    print("\n    ğŸ“‹ ä¸»é¢˜çº§æŒ‡æ ‡è¯¦è¡¨ï¼ˆæŒ‰å‡æ–°é—»æ¯”ä¾‹æ’åºï¼ŒTOP 10ï¼‰ï¼š")
    print("    â”Œâ”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”")
    print("    â”‚ä¸»é¢˜â”‚æ ·æœ¬æ•°â”‚å‡æ–°é—»æ¯”ä¾‹(RF)â”‚å‡æ–°é—»æ¯”ä¾‹(LR)â”‚å¹³å‡è€¸äººæŒ‡æ•°  â”‚å¹³å‡æƒ…æ„Ÿåˆ†æ•°  â”‚")
    print("    â”œâ”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤")
    
    for idx, row in topic_analysis.nlargest(10, 'predicted_fake_ratio_rf').iterrows():
        print(f"    â”‚{int(row['lda_topic']):3d} â”‚{int(row['headline_count']):5d} â”‚"
              f"    {row['predicted_fake_ratio_rf']*100:5.1f}%    â”‚"
              f"    {row['predicted_fake_ratio_lr']*100:5.1f}%    â”‚"
              f"    {row['avg_sensationalism']:5.3f}    â”‚"
              f"   {row['avg_sentiment']:+6.3f}    â”‚")
    
    print("    â””â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜")
    
    # ==================== ä¿å­˜ç»“æœ ====================
    print("\n" + "â–¶"*40)
    print("ä¿å­˜ç»“æœæ–‡ä»¶")
    print("â–¶"*40)
    
    print("[1/3] ä¿å­˜é¢„æµ‹ç»“æœ...")
    df_pred[['headline_text', 'lda_topic', 'sensationalism_score', 'sentiment_compound',
             'fake_pred_rf', 'fake_prob_rf', 'fake_pred_lr', 'fake_prob_lr']].to_csv(
        'fake_news_predictions_improved.csv', index=False, encoding='utf-8'
    )
    print("    âœ… ä¿å­˜ï¼šfake_news_predictions_improved.csv")
    
    print("[2/3] ä¿å­˜ä¸»é¢˜çº§åˆ†æ...")
    topic_analysis.to_csv('topic_analysis_improved.csv', index=False, encoding='utf-8')
    print("    âœ… ä¿å­˜ï¼štopic_analysis_improved.csv")
    
    print("[3/3] ä¿å­˜è®­ç»ƒæ•°æ®...")
    training_df.to_csv('training_data_used.csv', index=False, encoding='utf-8')
    print("    âœ… ä¿å­˜ï¼štraining_data_used.csv")

    # ==================== æœ€ç»ˆæ€»ç»“ ====================
    print("\n" + "="*80)
    print("ğŸ‰ åˆ†æå®Œæˆï¼æœ€ç»ˆæ€»ç»“")
    print("="*80)
    
    total_time = train_time_rf + train_time_lr
    
    print(f"""
âš¡ è¿è¡Œæ•ˆç‡ç»Ÿè®¡ï¼ˆé‡‡æ ·æ¨¡å¼ {sampling_ratio*100:.0f}%ï¼‰ï¼š
   â€¢ æ•°æ®é‡‡æ ·æ¯”ä¾‹ï¼š{sampling_ratio*100:.0f}% (ä»Fake.csvå’ŒTrue.csvä¸­é‡‡æ ·)
   â€¢ ç‰¹å¾è®¡ç®—è€—æ—¶ï¼š~2-5åˆ†é’Ÿ
   â€¢ æ¨¡å‹è®­ç»ƒè€—æ—¶ï¼š{total_time:.1f}s
   â€¢ é¢„æµ‹+åˆ†æè€—æ—¶ï¼š~1-2åˆ†é’Ÿ
   â€¢ ğŸš€ æ€»ä½“é€Ÿåº¦æå‡ï¼šçº¦5å€ï¼ˆç›¸æ¯”å…¨é‡æ•°æ®ï¼‰

ğŸ“Š æ•°æ®ç»Ÿè®¡ï¼š
   â€¢ ä¸»æ•°æ®é›†ï¼š{len(df_pred):,} æ¡æ•°æ®
   â€¢ æ ‡æ³¨æ•°æ®ï¼š{len(training_df)} æ¡ï¼ˆç”¨äºè®­ç»ƒï¼‰
   â€¢ ä¸»é¢˜æ•°é‡ï¼š{len(topic_analysis)} ä¸ªæœ‰æ•ˆä¸»é¢˜
   
ğŸ”§ æ¨¡å‹æ€§èƒ½ï¼š
   â€¢ éšæœºæ£®æ— - å‡†ç¡®ç‡ {rf_train_acc:.2%}ï¼ŒF1åˆ†æ•° {rf_train_f1:.4f}
   â€¢ é€»è¾‘å›å½’ - å‡†ç¡®ç‡ {lr_train_acc:.2%}ï¼ŒF1åˆ†æ•° {lr_train_f1:.4f}
   
ğŸ“ˆ æ ¸å¿ƒå‘ç°ï¼š
   â€¢ è€¸äººæŒ‡æ•°ä¸å‡æ–°é—»ç›¸å…³æ€§ï¼ˆRFï¼‰ï¼šr = {corr_rf:+.4f}ï¼Œp = {p_rf:.4f}
   â€¢ è€¸äººæŒ‡æ•°ä¸å‡æ–°é—»ç›¸å…³æ€§ï¼ˆLRï¼‰ï¼šr = {corr_lr:+.4f}ï¼Œp = {p_lr:.4f}
   â€¢ é¢„æµ‹å‡æ–°é—»æ¯”ä¾‹ï¼ˆRFï¼‰ï¼š{df_pred['fake_pred_rf'].mean()*100:.1f}%
   â€¢ é¢„æµ‹å‡æ–°é—»æ¯”ä¾‹ï¼ˆLRï¼‰ï¼š{df_pred['fake_pred_lr'].mean()*100:.1f}%
   
ğŸ’¾ è¾“å‡ºæ–‡ä»¶ï¼š
   1. fake_news_predictions_improved.csv - å®Œæ•´é¢„æµ‹ç»“æœ
   2. topic_analysis_improved.csv - ä¸»é¢˜çº§æŒ‡æ ‡
   3. training_data_used.csv - è®­ç»ƒæ•°æ®
   
âœ¨ ä¸‹ä¸€æ­¥å»ºè®®ï¼š
   â€¢ æ£€æŸ¥ topic_analysis_improved.csv ä¸­ç›¸å…³æ€§æœ€å¼ºçš„ä¸»é¢˜
   â€¢ åœ¨æŠ¥å‘Šä¸­çªå‡ºæ˜¾ç¤ºæ¨¡å‹çš„ç‰¹å¾é‡è¦æ€§æ’åº
   â€¢ å¯¹æ¯”RFå’ŒLRä¸¤ä¸ªæ¨¡å‹çš„é¢„æµ‹å·®å¼‚ï¼Œè®¨è®ºæ¨¡å‹ä¼˜åŠ£
   â€¢ å¦‚éœ€å®Œæ•´æ•°æ®ï¼Œæ”¹å‚æ•°ï¼šsampling_ratio=1.0ï¼ˆä½¿ç”¨100%æ•°æ®ï¼‰
    """)
    
    print("="*80)
    print("æ„Ÿè°¢ä½¿ç”¨ï¼ğŸ“§ å¦‚æœ‰é—®é¢˜ï¼Œè¯·æ£€æŸ¥è¾“å‡ºæ–‡ä»¶ä¸­çš„è¯¦ç»†æ•°æ®")
    print("="*80 + "\n")
    
    return df_pred, topic_analysis, training_df


# ===================== æ‰§è¡Œå…¥å£ =====================
if __name__ == "__main__":
    # ã€å¿«é€Ÿæµ‹è¯•ç‰ˆæœ¬ã€‘ï¼šè‡ªåŠ¨é‡‡æ ·1/5æ•°æ®ï¼ˆsampling_ratio=0.2ï¼‰
    print("\n" + "âš¡"*40)
    print("ğŸ”¥ å¿«é€Ÿæµ‹è¯•æ¨¡å¼å¯åŠ¨ï¼ˆé‡‡æ ·1/5æ•°æ®ï¼Œé€Ÿåº¦å¿«5å€ï¼‰")
    print("âš¡"*40)
    
    df_pred, topic_analysis, training_df = improved_fake_news_detection_ml_based(
        sensationalism_pkl='full_data_with_sensationalism_score.pkl',
        fake_news_csv='Fake.csv',
        real_news_csv='True.csv',
        threshold=65,
        sample_size=None,  # None = ä½¿ç”¨å…¨éƒ¨ä¸»æ•°æ®é›†
        sampling_ratio=1,  # ã€å…³é”®å‚æ•°ã€‘ä»CSVé‡‡æ ·æ¯”ä¾‹ï¼Œ0.2 = 1/5ï¼Œå¯æ”¹ä¸º 0.5/1.0
        main_data_sampling_ratio = 1  # ã€æ–°å¢ã€‘ä¸»æ•°æ®é›†é‡‡æ ·æ¯”ä¾‹ï¼ˆé»˜è®¤1/10ï¼‰
    )
    
    # ã€ä½¿ç”¨å®Œæ•´æ•°æ®ã€‘ï¼šå–æ¶ˆä¸‹é¢æ³¨é‡Šï¼Œæ”¹ä¸º sampling_ratio=1.0
    # df_pred, topic_analysis, training_df = improved_fake_news_detection_ml_based(
    #     sensationalism_pkl='full_data_with_sensationalism_score.pkl',
    #     fake_news_csv='Fake.csv',
    #     real_news_csv='True.csv',
    #     threshold=65,
    #     sample_size=None,
    #     sampling_ratio=1.0  # æ”¹ä¸º 1.0 ä½¿ç”¨100%æ•°æ®
    # )
