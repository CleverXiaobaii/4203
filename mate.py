# @title
import numpy as np
import pandas as pd
from IPython.display import display
from tqdm import tqdm
from collections import Counter
import ast

# æ–¹æ³•ä¸€ï¼šè®¾ç½®Matplotlibæ— GUIåç«¯ï¼ˆå¿…é¡»åœ¨import pltä¹‹å‰ï¼‰
import matplotlib

matplotlib.use('Agg')  # æ— GUIåç«¯ï¼Œåªä¿å­˜å›¾ç‰‡ä¸æ˜¾ç¤º
import matplotlib.pyplot as plt
import matplotlib.mlab as mlab
import seaborn as sb

from sklearn.feature_extraction.text import CountVectorizer
# from textblob import TextBlob
import scipy.stats as stats

from sklearn.decomposition import TruncatedSVD
from sklearn.decomposition import LatentDirichletAllocation as LDA
from sklearn.manifold import TSNE

from bokeh.plotting import figure, output_file, show
from bokeh.models import Label
from bokeh.io import output_notebook

output_notebook()

from collections import Counter
import re
import nltk
from nltk.corpus import stopwords
from nltk.util import ngrams
from sklearn.feature_extraction.text import CountVectorizer
from wordcloud import WordCloud
from sklearn.feature_extraction.text import TfidfVectorizer

from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score

import joblib

# %matplotlib inline

nltk.download('stopwords', quiet=True)
nltk.download('vader_lexicon')

# ===================== ç»Ÿä¸€åŠ è½½æ•°æ®ï¼Œå…¨ç¨‹ä½¿ç”¨åŒä¸€ä¸ªDataFrame =====================
# ä¼˜å…ˆåŠ è½½å·²å¤„ç†æ•°æ®ï¼Œæ— åˆ™åŠ è½½åŸå§‹æ•°æ®å¹¶å¤„ç†
datafile = 'abcnews-date-text.csv'
try:
    # å°è¯•åŠ è½½æœ€ç»ˆå¤„ç†æ•°æ®ï¼ˆå¦‚æœä¹‹å‰è¿è¡Œè¿‡ï¼‰
    df = pd.read_pickle('processed_eda_sentiment_df.pkl')
    print("Loaded processed DataFrame with sentiment from pickle.")
    # è¡¥å…¨å¯èƒ½ç¼ºå¤±çš„åˆ—
    if 'word_count' not in df.columns:
        df['word_count'] = df['headline_text'].str.split().str.len()
    if 'char_count' not in df.columns:
        df['char_count'] = df['headline_text'].str.len()
    if 'year' not in df.columns:
        df['year'] = df['publish_date'].dt.year
    if 'month' not in df.columns:
        df['month'] = df['publish_date'].dt.month
except FileNotFoundError:
    # åŠ è½½åŸå§‹æ•°æ®å¹¶å¤„ç†
    print("Loading raw data and processing...")
    raw_data = pd.read_csv(datafile, parse_dates=[0])  # ç§»é™¤åºŸå¼ƒå‚æ•°
    df = raw_data.head(50000).copy()

    # åŸºç¡€æ¸…æ´—
    df['headline_text'] = df['headline_text'].str.lower().str.replace(r'[^\w\s]', '', regex=True)

    # æ–‡æœ¬é•¿åº¦ç‰¹å¾
    df['word_count'] = df['headline_text'].str.split().str.len()  # è¯æ•°
    df['char_count'] = df['headline_text'].str.len()  # å­—ç¬¦æ•°

    # æ—¶é—´ç‰¹å¾
    df['year'] = df['publish_date'].dt.year
    df['month'] = df['publish_date'].dt.month

    print(f"Dataset loaded and processed: {len(df)} headlines")

# ä¿å­˜ä¸­é—´æ•°æ®ï¼ˆä»…ä¸€æ¬¡ï¼‰
df.to_pickle('processed_eda_df.pkl')
print("Intermediate DataFrame saved as 'processed_eda_df.pkl'")

# ===================== æ•°æ®åŸºæœ¬ç»Ÿè®¡ =====================
print("\n=== æ•¸æ“šåŸºæœ¬çµ±è¨ˆ (Basic Dataset Stats) ===")
print(f"ç¸½ headlines æ•¸: {len(df):,}")
print(f"å¹³å‡è©æ•¸: {df['word_count'].mean():.2f} (std: {df['word_count'].std():.2f})")
print(f"å¹³å‡å­—ç¬¦æ•¸: {df['char_count'].mean():.2f} (std: {df['char_count'].std():.2f})")
print(f"è©æ•¸ç¯„åœ: {df['word_count'].min()} - {df['word_count'].max()}")
print(f"å­—ç¬¦æ•¸ç¯„åœ: {df['char_count'].min()} - {df['char_count'].max()}")

# è¯æ•°/å­—ç¬¦æ•°åˆ†å¸ƒå¯è§†åŒ–
fig, axes = plt.subplots(1, 2, figsize=(12, 4))
df['word_count'].hist(bins=20, ax=axes[0], edgecolor='black', color='skyblue')
axes[0].set_title('Word Count Distribution')
axes[0].set_xlabel('Number of Words')
axes[0].set_ylabel('Frequency')

df['char_count'].hist(bins=20, ax=axes[1], edgecolor='black', color='lightgreen')
axes[1].set_title('Character Count Distribution')
axes[1].set_xlabel('Number of Characters')
axes[1].set_ylabel('Frequency')
plt.tight_layout()
plt.savefig('word_char_distribution.png', dpi=150, bbox_inches='tight')
print("å›¾ç‰‡å·²ä¿å­˜ï¼šword_char_distribution.png")

# ===================== æ—¶é—´è¶‹åŠ¿åˆ†æ =====================
reindexed_data = df['headline_text'].copy()
reindexed_data.index = df['publish_date']

monthly_counts = reindexed_data.resample('M').count()
yearly_counts = reindexed_data.resample('A').count()
daily_counts = reindexed_data.resample('D').count()

fig, ax = plt.subplots(3, figsize=(18, 16))
ax[0].plot(daily_counts);
ax[0].set_title('Daily Counts');
ax[1].plot(monthly_counts);
ax[1].set_title('Monthly Counts');
ax[2].plot(yearly_counts);
ax[2].set_title('Yearly Counts');
plt.tight_layout()
plt.savefig('temporal_trends.png', dpi=150, bbox_inches='tight')
print("å›¾ç‰‡å·²ä¿å­˜ï¼štemporal_trends.png")

# ===================== æƒ…æ„Ÿåˆ†æ =====================
# å®‰è£…å¹¶å¯¼å…¥VADER
try:
    from nltk.sentiment.vader import SentimentIntensityAnalyzer
except ImportError:
    import subprocess
    import sys

    subprocess.check_call([sys.executable, "-m", "pip", "install", "vaderSentiment"])
    from nltk.sentiment.vader import SentimentIntensityAnalyzer

# å…³é”®ä¿®å¤ï¼šæå‰å®šä¹‰analyzerï¼Œç¡®ä¿å…¨å±€å¯ç”¨
analyzer = SentimentIntensityAnalyzer()

# è®¡ç®—æƒ…æ„Ÿåˆ†æ•°ï¼ˆç¡®ä¿åªè®¡ç®—ä¸€æ¬¡ï¼‰
if 'sentiment_compound' not in df.columns:
    df['sentiment_compound'] = df['headline_text'].apply(lambda x: analyzer.polarity_scores(x)['compound'])
    df['sentiment_pos'] = df['headline_text'].apply(lambda x: analyzer.polarity_scores(x)['pos'])
    df['sentiment_neg'] = df['headline_text'].apply(lambda x: analyzer.polarity_scores(x)['neg'])
    df['sentiment_neu'] = df['headline_text'].apply(lambda x: analyzer.polarity_scores(x)['neu'])
    # ä¿å­˜åŒ…å«æƒ…æ„Ÿåˆ†æ•°çš„æ•°æ®
    df.to_pickle('processed_eda_sentiment_df.pkl')
    print("DataFrame with sentiment saved as 'processed_eda_sentiment_df.pkl'")

# æƒ…æ„Ÿç»Ÿè®¡
print("\n=== æƒ…æ„ŸåŸºç·šçµ±è¨ˆ (Sentiment Baseline Stats) ===")
print(df['sentiment_compound'].describe())
print(f"å¹³å‡æƒ…æ„Ÿåˆ†æ•¸: {df['sentiment_compound'].mean():.3f} (è² é¢åå¤š?)")
print(f"æ­£å‘æ¯”ä¾‹ (>0.05): {(df['sentiment_compound'] > 0.05).mean():.1%}")
print(f"ä¸­æ€§æ¯”ä¾‹ (-0.05~0.05): {((df['sentiment_compound'] >= -0.05) & (df['sentiment_compound'] <= 0.05)).mean():.1%}")
print(f"è² å‘æ¯”ä¾‹ (<-0.05): {(df['sentiment_compound'] < -0.05).mean():.1%}")

# æƒ…æ„Ÿåˆ†å¸ƒå¯è§†åŒ–
plt.figure(figsize=(10, 5))
plt.subplot(1, 2, 1)
df['sentiment_compound'].hist(bins=30, edgecolor='black', color='lightblue', alpha=0.7)
plt.title('Sentiment Compound Score Distribution')
plt.xlabel('Compound Score (-1 to +1)')
plt.ylabel('Frequency')
plt.axvline(df['sentiment_compound'].mean(), color='red', linestyle='--',
            label=f'Mean: {df["sentiment_compound"].mean():.3f}')
plt.legend()

# æƒ…æ„Ÿææ€§é¥¼å›¾
plt.subplot(1, 2, 2)
polarity_counts = df['sentiment_compound'].apply(
    lambda x: 'Positive' if x > 0.05 else ('Negative' if x < -0.05 else 'Neutral')).value_counts()
plt.pie(polarity_counts.values, labels=polarity_counts.index, autopct='%1.1f%%', startangle=90)
plt.title('Sentiment Polarity Proportions')
plt.axis('equal')
plt.tight_layout()
plt.savefig('sentiment_distribution.png', dpi=150, bbox_inches='tight')
print("å›¾ç‰‡å·²ä¿å­˜ï¼šsentiment_distribution.png")

# ===================== æ­£è´Ÿå‘é«˜é¢‘è¯åˆ†æ =====================
positive_words = Counter()
negative_words = Counter()
stop_words = set(stopwords.words('english'))

for headline in df['headline_text']:
    words = headline.split()
    scores = analyzer.polarity_scores(headline)  # ç°åœ¨analyzerä¸€å®šå·²å®šä¹‰
    if scores['compound'] > 0.05:
        pos_words = [w for w in words if w not in stop_words and len(w) > 2]
        positive_words.update(pos_words)
    elif scores['compound'] < -0.05:
        neg_words = [w for w in words if w not in stop_words and len(w) > 2]
        negative_words.update(neg_words)

# è¾“å‡ºTop10æ­£è´Ÿå‘è¯
top_pos = positive_words.most_common(10)
top_neg = negative_words.most_common(10)

print(f"\n=== Top 10 Positive Words (from positive headlines) ===")
for word, count in top_pos:
    print(f"{word}: {count}")

print(f"\n=== Top 10 Negative Words (from negative headlines) ===")
for word, count in top_neg:
    print(f"{word}: {count}")

# å¯è§†åŒ–æ­£è´Ÿå‘é«˜é¢‘è¯
fig, axes = plt.subplots(1, 2, figsize=(12, 5))
pos_df = pd.DataFrame(top_pos, columns=['word', 'count'])
pos_df.plot.bar(x='word', y='count', ax=axes[0], color='green')
axes[0].set_title('Top Positive Words')
axes[0].set_xlabel('Words')
axes[0].set_ylabel('Frequency')
axes[0].tick_params(axis='x', rotation=45)

neg_df = pd.DataFrame(top_neg, columns=['word', 'count'])
neg_df.plot.bar(x='word', y='count', ax=axes[1], color='red')
axes[1].set_title('Top Negative Words')
axes[1].set_xlabel('Words')
axes[1].set_ylabel('Frequency')
axes[1].tick_params(axis='x', rotation=45)
plt.tight_layout()
plt.savefig('top_sentiment_words.png', dpi=150, bbox_inches='tight')
print("å›¾ç‰‡å·²ä¿å­˜ï¼štop_sentiment_words.png")

# ===================== æƒ…æ„Ÿç¤ºä¾‹ =====================
print("\n=== ç¤ºä¾‹ Headlines (Negative vs Positive) ===")
neg_examples = df[df['sentiment_compound'] < -0.05]['headline_text'].head(3).tolist()
pos_examples = df[df['sentiment_compound'] > 0.05]['headline_text'].head(3).tolist()

print("Negative Examples:")
for ex in neg_examples:
    print(f"  - {ex} (score: {analyzer.polarity_scores(ex)['compound']:.3f})")

print("\nPositive Examples:")
for ex in pos_examples:
    print(f"  - {ex} (score: {analyzer.polarity_scores(ex)['compound']:.3f})")

# ===================== æ•°æ®è´¨é‡æ£€æŸ¥ =====================
print("\n=== ç¼ºå¤±å€¼æ£€æŸ¥ (Missing Values Check) ===")
missing_stats = df.isnull().sum()
print(missing_stats)
print(f"ç¸½ç¼ºå¤±å€¼: {missing_stats.sum()}")
print(f"ç¼ºå¤±æ¯”ä¾‹: {missing_stats.sum() / len(df) * 100:.2f}%")

# ç¼ºå¤±å€¼çƒ­åŠ›å›¾ï¼ˆå¦‚æœ‰ç¼ºå¤±ï¼‰
if missing_stats.sum() > 0:
    plt.figure(figsize=(8, 4))
    sb.heatmap(df.isnull(), yticklabels=False, cbar=True, cmap='viridis')
    plt.title('Missing Values Heatmap')
    plt.savefig('missing_values_heatmap.png', dpi=150, bbox_inches='tight')
    print("å›¾ç‰‡å·²ä¿å­˜ï¼šmissing_values_heatmap.png")
else:
    print("No missing values found - good data quality!")

# å»é‡ï¼ˆç¡®ä¿æ•°æ®å”¯ä¸€æ€§ï¼‰
df = df.drop_duplicates(subset=['headline_text']).reset_index(drop=True)
print(f"å»é‡åæ•°æ®é‡ï¼š{len(df)} æ¡ headlines")

# ===================== TF-IDF å‘é‡åŒ– =====================
vectorizer = TfidfVectorizer(
    max_features=5000,
    ngram_range=(1, 2),  # å•å­—+åŒå­—çŸ­è¯­
    stop_words='english',
    min_df=2,  # å¿½ç•¥å‡ºç°æ¬¡æ•°<2çš„è¯
    max_df=0.95  # å¿½ç•¥å‡ºç°é¢‘ç‡>95%çš„è¯
)

X_tfidf = vectorizer.fit_transform(df['headline_text'])
feature_names = vectorizer.get_feature_names_out()

print("\n=== TF-IDF åŸºæœ¬çµ±è¨ˆ (TF-IDF Basic Stats) ===")
print(f"TF-IDF Matrix Shape: {X_tfidf.shape} (docs x features)")
print(f"Sparsity (non-zero %): {X_tfidf.nnz / (X_tfidf.shape[0] * X_tfidf.shape[1]) * 100:.2f}%")
print(f"Vocabulary Size: {len(feature_names):,}")

# Top20 TF-IDF æœ¯è¯­
mean_tf_idf = np.asarray(X_tfidf.mean(axis=0)).flatten()
top_indices = mean_tf_idf.argsort()[-20:][::-1]
top_terms = [feature_names[i] for i in top_indices]
top_scores = mean_tf_idf[top_indices]

print(f"\n=== Top 20 TF-IDF Terms (Term Importance) ===")
for term, score in zip(top_terms, top_scores):
    print(f"{term}: {score:.4f}")

# å¯è§†åŒ–Top20 TF-IDFæœ¯è¯­
top_df = pd.DataFrame({'term': top_terms, 'score': top_scores})
plt.figure(figsize=(12, 6))
top_df.plot.bar(x='term', y='score', color='blue')
plt.title('Top 20 TF-IDF Terms (Quantifying Term Importance)')
plt.xlabel('Terms (Unigrams + Bigrams)')
plt.ylabel('Mean TF-IDF Score')
plt.xticks(rotation=45)
plt.tight_layout()
plt.savefig('top_tfidf_terms.png', dpi=150, bbox_inches='tight')
print("å›¾ç‰‡å·²ä¿å­˜ï¼štop_tfidf_terms.png")

# ===================== LDA ä¸»é¢˜èšç±» =====================
n_topics = 10  # 10ä¸ªä¸»é¢˜
lda = LDA(
    n_components=n_topics,
    random_state=42,
    max_iter=10,
    evaluate_every=-1
)

X_lda = lda.fit_transform(X_tfidf)
df['lda_topic'] = X_lda.argmax(axis=1)  # ä¸ºæ¯æ¡æ•°æ®åˆ†é…æœ€å¯èƒ½çš„ä¸»é¢˜

# è¾“å‡ºæ¯ä¸ªä¸»é¢˜çš„å…³é”®è¯ï¼ˆå¸®åŠ©ç†è§£ä¸»é¢˜å«ä¹‰ï¼‰
print(f"\n=== LDA ä¸»é¢˜å…³é”®è¯ (Top 5 words per topic) ===")
for topic_idx, topic in enumerate(lda.components_):
    top_words = [feature_names[i] for i in topic.argsort()[:-6:-1]]  # æ¯ä¸ªä¸»é¢˜Top5è¯
    print(f"ä¸»é¢˜ {topic_idx}: {' | '.join(top_words)}")

# ===================== ä¿å­˜æœ€ç»ˆæ•°æ®ï¼ˆåŒ…å«LDAèšç±»å’Œæƒ…æ„Ÿåˆ†æ•°ï¼‰ =====================
# ç¡®è®¤æ‰€æœ‰å¿…è¦åˆ—å­˜åœ¨
required_columns = [
    'publish_date', 'headline_text', 'word_count', 'char_count', 'year', 'month',
    'sentiment_compound', 'sentiment_pos', 'sentiment_neg', 'sentiment_neu',
    'lda_topic'
]

# è¡¥å…¨å¯èƒ½ç¼ºå¤±çš„åˆ—ï¼ˆé˜²å¾¡æ€§ç¼–ç¨‹ï¼‰
for col in required_columns:
    if col not in df.columns:
        if col == 'word_count':
            df[col] = df['headline_text'].str.split().str.len()
        elif col == 'char_count':
            df[col] = df['headline_text'].str.len()
        elif col in ['year', 'month']:
            df[col] = df['publish_date'].dt.__getattribute__(col)
        print(f"âš ï¸  è¡¥å…¨ç¼ºå¤±åˆ—ï¼š{col}")

# æ‰“å°ä¿å­˜çš„åˆ—ä¿¡æ¯
print("\n=== æœ€ç»ˆä¿å­˜çš„åˆ—ä¿¡æ¯ ===")
for col in required_columns:
    print(f"âœ… {col} (æ•°æ®ç±»å‹: {df[col].dtype})")

# åŒæ ¼å¼ä¿å­˜
# 1. Pickleæ ¼å¼ï¼ˆä¿ç•™å®Œæ•´æ•°æ®ç±»å‹ï¼Œæ¨èåç»­åˆ†æä½¿ç”¨ï¼‰
df.to_pickle('final_data_with_topic_sentiment.pkl')
# 2. CSVæ ¼å¼ï¼ˆé€šç”¨æ€§å¼ºï¼Œæ–¹ä¾¿æŸ¥çœ‹åˆ†äº«ï¼‰
df.to_csv('final_data_with_topic_sentiment.csv', index=False, encoding='utf-8')

# ä¿å­˜LDAæ¨¡å‹å’ŒTF-IDFå‘é‡å™¨ï¼ˆæ–¹ä¾¿åç»­å¤ç”¨ï¼‰
joblib.dump(lda, 'lda_topic_model.pkl')
joblib.dump(vectorizer, 'tfidf_vectorizer.pkl')

# è¾“å‡ºä¿å­˜ç»“æœ
print(f"\nğŸ“Š æ•°æ®ä¿å­˜å®Œæˆï¼")
print(f"ğŸ“ ä¸»æ•°æ®æ–‡ä»¶ï¼š")
print(f"   - Pickle: final_data_with_topic_sentiment.pkl (ä¿ç•™datetimeç­‰ç±»å‹)")
print(f"   - CSV: final_data_with_topic_sentiment.csv (é€šç”¨æ ¼å¼)")
print(f"ğŸ“ æ¨¡å‹æ–‡ä»¶ï¼š")
print(f"   - LDAæ¨¡å‹: lda_topic_model.pkl")
print(f"   - TF-IDFå‘é‡å™¨: tfidf_vectorizer.pkl")
print(f"ğŸ“ˆ æœ€ç»ˆæ•°æ®é‡ï¼š{len(df)} æ¡ headlines")

# è¾“å‡ºLDAä¸»é¢˜åˆ†å¸ƒ
print(f"\n=== LDA ä¸»é¢˜åˆ†å¸ƒ ===")
topic_dist = df['lda_topic'].value_counts().sort_index()
for topic_id, count in topic_dist.items():
    print(f"ä¸»é¢˜ {topic_id}: {count} æ¡ ({count / len(df) * 100:.1f}%)")