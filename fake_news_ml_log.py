# ===================== æ”¹è¿›ç‰ˆæœ¬ï¼šä½¿ç”¨è¯­ä¹‰ç›¸ä¼¼åº¦ + æœºå™¨å­¦ä¹ åˆ†ç±» =====================
# ç‰ˆæœ¬ï¼šv2.5 - å¯¹æ•°ç›¸å…³ä¿®æ­£ç‰ˆï¼ˆlogè€¸äººæŒ‡æ•° vs åŸå§‹å‡æ–°é—»ç‡ï¼‰
import numpy as np
import pandas as pd
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report
from sklearn.preprocessing import StandardScaler
import matplotlib.pyplot as plt
import seaborn as sns
from scipy.stats import pearsonr
import re
import time


# è¿›åº¦æ¡ç±»ï¼ˆç¾åŒ–è¾“å‡ºï¼‰
class ProgressBar:
    def __init__(self, total, desc="Processing"):
        self.total = total
        self.desc = desc
        self.current = 0
        self.start_time = time.time()

    def update(self, n=1):
        self.current += n
        percent = self.current / self.total
        elapsed = time.time() - self.start_time
        rate = self.current / elapsed if elapsed > 0 else 0
        remaining = (self.total - self.current) / rate if rate > 0 else 0

        bar_length = 50
        filled = int(bar_length * percent)
        bar = 'â–ˆ' * filled + 'â–‘' * (bar_length - filled)

        print(f"\r[{self.desc}] {bar} {percent * 100:5.1f}% ({self.current}/{self.total}) | "
              f"Elapsed: {int(elapsed)}s | Remaining: {int(remaining)}s", end='', flush=True)

    def finish(self):
        print()  # æ¢è¡Œ


def improved_fake_news_detection_ml_based(
        sensationalism_pkl='full_data_with_sensationalism_score.pkl',
        fake_news_csv='Fake.csv',
        real_news_csv='True.csv',
        threshold=65,
        sample_size=None,  # å¯é€‰ï¼šåªå¤„ç†å‰Næ¡æ•°æ®ï¼Œç”¨äºå¿«é€Ÿæµ‹è¯•
        sampling_ratio=0.1,  # ã€ä¿®æ”¹ã€‘é»˜è®¤1/10æ•°æ®é‡‡æ ·ï¼ˆ0.1 = 1/10ï¼‰
        main_data_sampling_ratio=0.1,  # ã€æ–°å¢ã€‘ä¸»æ•°æ®é›†é‡‡æ ·æ¯”ä¾‹ï¼ˆé»˜è®¤1/10ï¼‰
        log_transform_offset=1e-3  # å¯¹æ•°å˜æ¢åç§»é‡ï¼Œé¿å…0å€¼å’Œè´Ÿå€¼
):
    """
    æ”¹è¿›ç‰ˆå‡æ–°é—»æ£€æµ‹ï¼ˆå¸¦è¯¦ç»†è¿›åº¦æç¤ºï¼‰ï¼š
    æ ¸å¿ƒæ€è·¯ï¼šä¸ç›´æ¥ç”¨ç›¸ä¼¼åº¦åˆ¤æ–­ï¼Œè€Œæ˜¯æ„å»ºç‰¹å¾å‘é‡ â†’ è®­ç»ƒæœºå™¨å­¦ä¹ æ¨¡å‹
    è¿™æ ·èƒ½åˆ©ç”¨å¤šç‰¹å¾ç»„åˆï¼Œè€Œä¸æ˜¯ä¾èµ–å•ä¸€çš„ç›¸ä¼¼åº¦æŒ‡æ ‡

    ã€v2.5ç‰ˆæœ¬æ›´æ–°ã€‘
    - æ ¸å¿ƒä¿®æ”¹ï¼šå¯¹æ•°ç›¸å…³è®¡ç®—æ”¹ä¸ºã€Œlogè€¸äººå¬é—»æŒ‡æ•° vs åŸå§‹å‡æ–°é—»ç‡ã€
    - ä¸å†å¯¹å‡æ–°é—»ç‡è¿›è¡Œå¯¹æ•°å˜æ¢ï¼Œä¿æŒå‡æ–°é—»ç‡çš„åŸå§‹å«ä¹‰ï¼ˆ0-1æ¯”ä¾‹ï¼‰
    - ä»…å¯¹è€¸äººå¬é—»æŒ‡æ•°åšå¯¹æ•°å˜æ¢ï¼Œè§£å†³æ•°æ®åˆ†å¸ƒåæ–œé—®é¢˜
    - å…¶ä»–åŠŸèƒ½ä¿æŒä¸å˜ï¼ˆ1/10é‡‡æ ·åŠ é€Ÿï¼‰

    å‚æ•°ï¼š
        sampling_ratio: æ ‡æ³¨æ•°æ®ï¼ˆFake/True.csvï¼‰é‡‡æ ·æ¯”ä¾‹ï¼Œé»˜è®¤0.1ï¼ˆ1/10ï¼‰
                       å¯æ”¹ä¸º 0.2ï¼ˆ1/5ï¼‰ã€1.0ï¼ˆå…¨éƒ¨ï¼‰ç­‰
        main_data_sampling_ratio: ä¸»æ•°æ®é›†é‡‡æ ·æ¯”ä¾‹ï¼Œé»˜è®¤0.1ï¼ˆ1/10ï¼‰
                                 å¯æ”¹ä¸º 0.2ï¼ˆ1/5ï¼‰ã€1.0ï¼ˆå…¨éƒ¨ï¼‰ç­‰
        log_transform_offset: å¯¹æ•°å˜æ¢åç§»é‡ï¼Œé»˜è®¤1e-3
                             ç”¨äºå°†è€¸äººå¬é—»æŒ‡æ•°è½¬æ¢ä¸ºæ­£æ•°åè¿›è¡Œå¯¹æ•°å˜æ¢
    """
    from rapidfuzz import process, fuzz

    print("\n" + "=" * 80)
    print("=" * 80)
    print("        ğŸš€ æ”¹è¿›ç‰ˆå‡æ–°é—»æ£€æµ‹ç³»ç»Ÿï¼ˆåŸºäºæœºå™¨å­¦ä¹ åˆ†ç±»ï¼‰v2.5")
    print("        ğŸ” å¯¹æ•°ç›¸å…³ä¿®æ­£ç‰ˆ - logè€¸äººå¬é—»æŒ‡æ•° vs åŸå§‹å‡æ–°é—»ç‡")
    print("        âš¡ è¶…å¿«é€Ÿç‰ˆæœ¬ - ä»…ä½¿ç”¨1/10æ•°æ®ï¼Œè®¡ç®—é€Ÿåº¦æå‡10å€")
    print("=" * 80)
    print("=" * 80)

    # ==================== STEP 1: åŠ è½½åŸºç¡€æ•°æ®ï¼ˆå¸¦1/10é‡‡æ ·ï¼‰ ====================
    print("\n" + "â–¶" * 40)
    print("STEP 1/6: åŠ è½½åŸºç¡€æ•°æ®ï¼ˆ1/10é‡‡æ ·ï¼‰")
    print("â–¶" * 40)

    try:
        print("[1/3] æ­£åœ¨åŠ è½½ä¸»æ•°æ®é›†ï¼ˆå¸¦è€¸äººæŒ‡æ•°ï¼‰...")
        df_sens = pd.read_pickle(sensationalism_pkl)
        original_main_size = len(df_sens)

        # ä¸»æ•°æ®é›†é‡‡æ ·ï¼ˆ1/10ï¼‰
        if main_data_sampling_ratio < 1.0:
            df_sens = df_sens.sample(frac=main_data_sampling_ratio, random_state=42).reset_index(drop=True)
            print(
                f"    ğŸ“‰ ä¸»æ•°æ®é›†é‡‡æ ·ï¼š{len(df_sens):,} æ¡ / åŸå§‹ {original_main_size:,} æ¡ (é‡‡æ ·æ¯”ä¾‹ {main_data_sampling_ratio * 100:.0f}%)")

        if sample_size:
            df_sens = df_sens.iloc[:sample_size].copy()
            print(f"    âš ï¸  é™åˆ¶æ¨¡å¼ï¼šä»…å¤„ç†å‰ {sample_size} æ¡æ•°æ®")

        print(f"    âœ… æˆåŠŸåŠ è½½ï¼š{len(df_sens):,} æ¡æ•°æ®")
        print(f"    ğŸ“Š æ•°æ®åˆ—åï¼š{df_sens.columns.tolist()}")
    except Exception as e:
        print(f"    âŒ æ•°æ®åŠ è½½å¤±è´¥ï¼š{e}")
        return

    # ==================== STEP 2: åŠ è½½å’Œå¤„ç†æ ‡æ³¨æ•°æ®ï¼ˆ1/10é‡‡æ ·ï¼‰ ====================
    print("\n[2/3] æ­£åœ¨åŠ è½½æ ‡æ³¨æ•°æ®ï¼ˆå‡æ–°é—» + çœŸå®æ–°é—»ï¼‰...")

    def clean_text(text):
        """æ¸…æ´—æ–‡æœ¬"""
        text = str(text).strip().lower()
        text = re.sub(r'[^\w\s]', '', text)
        text = re.sub(r'\s+', ' ', text)
        return text

    try:
        # åŠ è½½å‡æ–°é—»ï¼ˆ1/10é‡‡æ ·ï¼‰
        print("    [2.1/3] åŠ è½½å‡æ–°é—»æ•°æ®...")
        fake_df = pd.read_csv(fake_news_csv)
        original_fake_size = len(fake_df)
        fake_df = fake_df.sample(frac=sampling_ratio, random_state=42).reset_index(drop=True)
        print(
            f"        âœ… åŠ è½½å‡æ–°é—» {len(fake_df):,} æ¡ / åŸå§‹ {original_fake_size:,} æ¡ (é‡‡æ ·æ¯”ä¾‹ {sampling_ratio * 100:.0f}%)")

        title_col_fake = [col for col in fake_df.columns if col.lower().strip() == 'title']
        if not title_col_fake:
            raise ValueError(f"å‡æ–°é—»CSVç¼ºå°‘'title'åˆ—ã€‚å½“å‰åˆ—ï¼š{fake_df.columns.tolist()}")
        fake_df = fake_df.rename(columns={title_col_fake[0]: 'headline'})
        fake_df['is_fake'] = 1

        # åŠ è½½çœŸå®æ–°é—»ï¼ˆ1/10é‡‡æ ·ï¼‰
        print("    [2.2/3] åŠ è½½çœŸå®æ–°é—»æ•°æ®...")
        real_df = pd.read_csv(real_news_csv)
        original_real_size = len(real_df)
        real_df = real_df.sample(frac=sampling_ratio, random_state=42).reset_index(drop=True)
        print(
            f"        âœ… åŠ è½½çœŸå®æ–°é—» {len(real_df):,} æ¡ / åŸå§‹ {original_real_size:,} æ¡ (é‡‡æ ·æ¯”ä¾‹ {sampling_ratio * 100:.0f}%)")

        title_col_real = [col for col in real_df.columns if col.lower().strip() == 'title']
        if not title_col_real:
            raise ValueError(f"çœŸå®æ–°é—»CSVç¼ºå°‘'title'åˆ—ã€‚å½“å‰åˆ—ï¼š{real_df.columns.tolist()}")
        real_df = real_df.rename(columns={title_col_real[0]: 'headline'})
        real_df['is_fake'] = 0

        # åˆå¹¶
        labeled_df = pd.concat([fake_df[['headline', 'is_fake']],
                                real_df[['headline', 'is_fake']]], ignore_index=True)

        # æ¸…æ´—å’Œå»é‡
        print("    [2.3/3] æ¸…æ´—å’Œå»é‡...")
        labeled_df['headline_clean'] = labeled_df['headline'].apply(clean_text)
        labeled_df = labeled_df[labeled_df['headline_clean'] != ""].drop_duplicates(subset=['headline_clean'])

        # ã€å…³é”®ä¿®å¤ã€‘é‡ç½®ç´¢å¼•ï¼Œç¡®ä¿ä¸åç»­åˆ—è¡¨ç´¢å¼•å¯¹é½
        labeled_df = labeled_df.reset_index(drop=True)

        print(f"        âœ… æ ‡æ³¨æ•°æ®åˆå¹¶å®Œæˆï¼š{len(labeled_df):,} æ¡")
        print(f"           - å‡æ–°é—»ï¼š{labeled_df['is_fake'].sum():,} æ¡")
        print(f"           - çœŸå®æ–°é—»ï¼š{(1 - labeled_df['is_fake']).sum():,} æ¡")
        print(f"           - å‡æ–°é—»æ¯”ä¾‹ï¼š{labeled_df['is_fake'].mean() * 100:.1f}%")

    except Exception as e:
        print(f"    âŒ æ ‡æ³¨æ•°æ®åŠ è½½å¤±è´¥ï¼š{e}")
        return

    # ==================== STEP 2: æ¸…æ´—ä¸»æ•°æ®é›† ====================
    print("\n" + "â–¶" * 40)
    print("STEP 2/6: æ¸…æ´—ä¸»æ•°æ®é›†")
    print("â–¶" * 40)

    print("[1/1] æ¸…æ´—ä¸»æ•°æ®é›†...")
    df_sens['headline_clean'] = df_sens['headline_text'].apply(clean_text)
    df_sens = df_sens[df_sens['headline_clean'] != ""].drop_duplicates(subset=['headline_clean'])
    df_sens = df_sens.reset_index(drop=True)  # é‡ç½®ç´¢å¼•
    print(f"    âœ… ä¸»æ•°æ®é›†æ¸…æ´—å®Œæˆï¼š{len(df_sens):,} æ¡æœ‰æ•ˆæ•°æ®")

    # ==================== STEP 3: ç‰¹å¾æå– ====================
    print("\n" + "â–¶" * 40)
    print("STEP 3/6: æå–å¤šç»´ç‰¹å¾")
    print("â–¶" * 40)

    print("ç‰¹å¾ç»´åº¦è¯´æ˜ï¼š")
    print("  1ï¸âƒ£  max_similarity - ä¸æ ‡æ³¨æ•°æ®çš„æœ€å¤§ç›¸ä¼¼åº¦ï¼ˆ0-1ï¼‰")
    print("  2ï¸âƒ£  sensationalism_score - è€¸äººå¬é—»æŒ‡æ•°ï¼ˆå·²æœ‰ï¼Œ0-1ï¼‰")
    print("  3ï¸âƒ£  headline_length_norm - æ ‡é¢˜é•¿åº¦å½’ä¸€åŒ–ï¼ˆ0-1ï¼‰")
    print("  4ï¸âƒ£  sentiment_extremity - æƒ…æ„Ÿæç«¯ç¨‹åº¦ï¼ˆ0-1ï¼‰")
    print("  5ï¸âƒ£  negative_bias - æ˜¯å¦ä¸ºè´Ÿé¢æ–°é—»ï¼ˆ0 or 1ï¼‰")

    # ç‰¹å¾4.1ï¼šæœ€é«˜ç›¸ä¼¼åº¦ï¼ˆè¯­ä¹‰ç›¸ä¼¼æ€§ï¼‰
    print("\n[1/5] è®¡ç®—ç›¸ä¼¼åº¦ç‰¹å¾...")
    labeled_texts = labeled_df['headline_clean'].tolist()

    def get_max_similarity_batch(texts, labeled_texts, batch_size=500):
        """æ‰¹é‡è®¡ç®—ç›¸ä¼¼åº¦ï¼Œå¸¦è¿›åº¦æç¤º"""
        similarities = []
        progress = ProgressBar(len(texts), desc="Similarity")

        for i in range(0, len(texts), batch_size):
            batch = texts[i:i + batch_size]
            for text in batch:
                if not text:
                    similarities.append(0)
                else:
                    try:
                        matches = process.extract(text, labeled_texts,
                                                  scorer=fuzz.token_sort_ratio, limit=1)
                        sim = matches[0][1] / 100 if matches else 0
                    except:
                        sim = 0
                    similarities.append(sim)
                progress.update(1)

        progress.finish()
        return similarities

    df_sens['max_similarity'] = get_max_similarity_batch(
        df_sens['headline_clean'].tolist(),
        labeled_texts,
        batch_size=500
    )
    print(f"    âœ… å®Œæˆ | ç›¸ä¼¼åº¦èŒƒå›´ï¼š{df_sens['max_similarity'].min():.3f} - {df_sens['max_similarity'].max():.3f}")

    # ç‰¹å¾4.2ï¼šå·²æœ‰çš„è€¸äººå¬é—»åˆ†æ•°ï¼ˆç›´æ¥å¤ç”¨ï¼‰
    print("[2/5] éªŒè¯è€¸äººå¬é—»ç‰¹å¾...")
    if 'sensationalism_score' not in df_sens.columns:
        print("    âš ï¸  è­¦å‘Šï¼šæ•°æ®ä¸­ç¼ºå°‘sensationalism_scoreåˆ—")
        print("    ä½¿ç”¨é»˜è®¤å€¼ 0.0")
        df_sens['sensationalism_score'] = 0.0
    else:
        print(
            f"    âœ… ç‰¹å¾å·²å­˜åœ¨ | èŒƒå›´ï¼š{df_sens['sensationalism_score'].min():.3f} - {df_sens['sensationalism_score'].max():.3f}")

    # ç‰¹å¾4.3ï¼šé•¿åº¦ç‰¹å¾
    print("[3/5] è®¡ç®—æ ‡é¢˜é•¿åº¦ç‰¹å¾...")
    progress = ProgressBar(len(df_sens), desc="Length")
    df_sens['headline_length'] = df_sens['headline_text'].apply(
        lambda x: len(str(x).split())
    )
    progress.update(len(df_sens))
    progress.finish()

    df_sens['headline_length_norm'] = (df_sens['headline_length'] - df_sens['headline_length'].min()) / \
                                      (df_sens['headline_length'].max() - df_sens['headline_length'].min() + 1e-8)
    print(f"    âœ… å®Œæˆ | é•¿åº¦èŒƒå›´ï¼š{df_sens['headline_length'].min()} - {df_sens['headline_length'].max()} è¯")

    # ç‰¹å¾4.4ï¼šæƒ…æ„Ÿæç«¯æ€§
    print("[4/5] è®¡ç®—æƒ…æ„Ÿæç«¯æ€§ç‰¹å¾...")
    progress = ProgressBar(len(df_sens), desc="Sentiment")
    df_sens['sentiment_extremity'] = df_sens['sentiment_compound'].apply(lambda x: abs(x))
    progress.update(len(df_sens))
    progress.finish()
    print(
        f"    âœ… å®Œæˆ | æç«¯æ€§èŒƒå›´ï¼š{df_sens['sentiment_extremity'].min():.3f} - {df_sens['sentiment_extremity'].max():.3f}")

    # ç‰¹å¾4.5ï¼šè´Ÿé¢å€¾å‘
    print("[5/5] è®¡ç®—è´Ÿé¢å€¾å‘ç‰¹å¾...")
    progress = ProgressBar(len(df_sens), desc="Negative")
    df_sens['negative_bias'] = df_sens['sentiment_compound'].apply(lambda x: 1 if x < -0.1 else 0)
    progress.update(len(df_sens))
    progress.finish()
    print(f"    âœ… å®Œæˆ | è´Ÿé¢æ¯”ä¾‹ï¼š{df_sens['negative_bias'].mean() * 100:.1f}%")

    # ==================== STEP 4: æ„å»ºè®­ç»ƒæ•°æ® ====================
    print("\n" + "â–¶" * 40)
    print("STEP 4/6: æ„å»ºè®­ç»ƒé›†ï¼ˆä»æ ‡æ³¨æ•°æ®ï¼‰")
    print("â–¶" * 40)

    print("[1/3] è®¡ç®—æ ‡æ³¨æ•°æ®çš„ç›¸ä¼¼åº¦ç‰¹å¾...")
    training_texts = labeled_df['headline_clean'].tolist()
    training_labels = labeled_df['is_fake'].tolist()

    training_max_sim = []
    progress = ProgressBar(len(training_texts), desc="Sim")

    for i, text in enumerate(training_texts):
        other_texts = [t for j, t in enumerate(training_texts) if j != i]
        if other_texts:
            try:
                matches = process.extract(text, other_texts,
                                          scorer=fuzz.token_sort_ratio, limit=1)
                sim = matches[0][1] / 100 if matches else 0
            except:
                sim = 0
        else:
            sim = 0
        training_max_sim.append(sim)
        progress.update(1)

    progress.finish()

    # ã€å…³é”®æ£€æŸ¥ã€‘ç¡®ä¿åˆ—è¡¨é•¿åº¦ä¸€è‡´
    if len(training_max_sim) != len(labeled_df):
        print(
            f"    âš ï¸  è­¦å‘Šï¼šé•¿åº¦ä¸åŒ¹é… | training_max_simé•¿åº¦={len(training_max_sim)} != labeled_dfé•¿åº¦={len(labeled_df)}")
        print(f"    æ­£åœ¨ä¿®å¤...")
        # å¦‚æœé•¿åº¦ä¸ä¸€è‡´ï¼Œæˆªæ–­æˆ–è¡¥å……
        if len(training_max_sim) > len(labeled_df):
            training_max_sim = training_max_sim[:len(labeled_df)]
        else:
            training_max_sim.extend([0] * (len(labeled_df) - len(training_max_sim)))

    print("[2/3] ä¸ºæ ‡æ³¨æ•°æ®åˆ†é…å…¶ä»–ç‰¹å¾...")
    # ã€æ”¹è¿›ã€‘ç›´æ¥ä½¿ç”¨indexed approachè€Œä¸æ˜¯iterrowsï¼ˆæ›´å¿«ä¸”é¿å…ç´¢å¼•é—®é¢˜ï¼‰
    training_data_list = []
    progress = ProgressBar(len(labeled_df), desc="Features")

    for idx in range(len(labeled_df)):
        # ç›´æ¥ç”¨ç´¢å¼•è®¿é—®
        row = labeled_df.iloc[idx]

        # åœ¨ä¸»æ•°æ®é›†ä¸­æŸ¥æ‰¾ç›¸åŒæ–‡æœ¬
        matching_rows = df_sens[df_sens['headline_clean'] == row['headline_clean']]

        if len(matching_rows) > 0:
            # å¦‚æœåœ¨ä¸»æ•°æ®é›†ä¸­æ‰¾åˆ°ï¼Œç›´æ¥å¤ç”¨ç‰¹å¾
            feat_row = matching_rows.iloc[0]
            training_data_list.append({
                'max_similarity': training_max_sim[idx],
                'sensationalism_score': feat_row.get('sensationalism_score', 0),
                'headline_length_norm': feat_row.get('headline_length_norm', 0),
                'sentiment_extremity': feat_row.get('sentiment_extremity', 0),
                'negative_bias': feat_row.get('negative_bias', 0),
                'is_fake': row['is_fake']
            })
        else:
            # å¦åˆ™è®¡ç®—ç‰¹å¾
            text_len = len(row['headline'].split())
            text_len_norm = (text_len - df_sens['headline_length'].min()) / \
                            (df_sens['headline_length'].max() - df_sens['headline_length'].min() + 1e-8)
            training_data_list.append({
                'max_similarity': training_max_sim[idx],
                'sensationalism_score': 0.5,  # é»˜è®¤ä¸­ç­‰
                'headline_length_norm': text_len_norm,
                'sentiment_extremity': 0.5,
                'negative_bias': 0,
                'is_fake': row['is_fake']
            })

        progress.update(1)

    progress.finish()

    print("[3/3] æ•´ç†è®­ç»ƒé›†...")
    training_df = pd.DataFrame(training_data_list)
    training_df = training_df.dropna()
    print(f"    âœ… è®­ç»ƒé›†æ„å»ºå®Œæˆï¼š{len(training_df)} æ¡")
    print(f"       - å‡æ–°é—»æ ·æœ¬ï¼š{training_df['is_fake'].sum()} æ¡ï¼ˆ{training_df['is_fake'].mean() * 100:.1f}%ï¼‰")
    print(f"       - çœŸå®æ–°é—»æ ·æœ¬ï¼š{len(training_df) - training_df['is_fake'].sum()} æ¡")

    # ==================== STEP 5: æ¨¡å‹è®­ç»ƒ ====================
    print("\n" + "â–¶" * 40)
    print("STEP 5/6: è®­ç»ƒæœºå™¨å­¦ä¹ æ¨¡å‹")
    print("â–¶" * 40)

    print("[1/4] æ ‡å‡†åŒ–ç‰¹å¾...")
    feature_cols = ['max_similarity', 'sensationalism_score', 'headline_length_norm',
                    'sentiment_extremity', 'negative_bias']

    scaler = StandardScaler()
    X_train = scaler.fit_transform(training_df[feature_cols])
    y_train = training_df['is_fake'].values
    print(f"    âœ… ç‰¹å¾æ ‡å‡†åŒ–å®Œæˆ | ç‰¹å¾ç»´åº¦ï¼š{X_train.shape}")

    # æ¨¡å‹1ï¼šéšæœºæ£®æ—
    print("\n[2/4] è®­ç»ƒéšæœºæ£®æ—æ¨¡å‹...")
    print("    â³ å¿«é€Ÿæ¨¡å¼ï¼šä»…éœ€ 2-8 ç§’ï¼ˆ1/10æ•°æ®ï¼‰...")

    rf_model = RandomForestClassifier(
        n_estimators=100,
        max_depth=10,
        random_state=42,
        n_jobs=-1,
        verbose=0
    )

    start_time = time.time()
    rf_model.fit(X_train, y_train)
    train_time_rf = time.time() - start_time

    rf_train_acc = accuracy_score(y_train, rf_model.predict(X_train))
    rf_train_prec = precision_score(y_train, rf_model.predict(X_train), zero_division=0)
    rf_train_rec = recall_score(y_train, rf_model.predict(X_train), zero_division=0)
    rf_train_f1 = f1_score(y_train, rf_model.predict(X_train), zero_division=0)

    print(f"    âœ… è®­ç»ƒå®Œæˆï¼ˆè€—æ—¶ {train_time_rf:.1f}sï¼‰")
    print(
        f"       ğŸ“Š è®­ç»ƒé›†æ€§èƒ½ï¼šå‡†ç¡®ç‡ {rf_train_acc:.2%} | ç²¾åº¦ {rf_train_prec:.2%} | å¬å› {rf_train_rec:.2%} | F1 {rf_train_f1:.2%}")

    # æ¨¡å‹2ï¼šé€»è¾‘å›å½’
    print("\n[3/4] è®­ç»ƒé€»è¾‘å›å½’æ¨¡å‹...")
    print("    â³ å¿«é€Ÿæ¨¡å¼ï¼šä»…éœ€ 1-3 ç§’ï¼ˆ1/10æ•°æ®ï¼‰...")

    lr_model = LogisticRegression(random_state=42, max_iter=200, verbose=0)

    start_time = time.time()
    lr_model.fit(X_train, y_train)
    train_time_lr = time.time() - start_time

    lr_train_acc = accuracy_score(y_train, lr_model.predict(X_train))
    lr_train_prec = precision_score(y_train, lr_model.predict(X_train), zero_division=0)
    lr_train_rec = recall_score(y_train, lr_model.predict(X_train), zero_division=0)
    lr_train_f1 = f1_score(y_train, lr_model.predict(X_train), zero_division=0)

    print(f"    âœ… è®­ç»ƒå®Œæˆï¼ˆè€—æ—¶ {train_time_lr:.1f}sï¼‰")
    print(
        f"       ğŸ“Š è®­ç»ƒé›†æ€§èƒ½ï¼šå‡†ç¡®ç‡ {lr_train_acc:.2%} | ç²¾åº¦ {lr_train_prec:.2%} | å¬å› {lr_train_rec:.2%} | F1 {lr_train_f1:.2%}")

    print("\n[4/4] æ¨¡å‹å¯¹æ¯”...")
    print("    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”")
    print("    â”‚ æ¨¡å‹            â”‚ å‡†ç¡®ç‡(%)  â”‚ ç²¾åº¦(%)    â”‚ å¬å›(%)    â”‚ F1-Score   â”‚")
    print("    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤")
    print(
        f"    â”‚ éšæœºæ£®æ—        â”‚ {rf_train_acc * 100:6.2f}     â”‚ {rf_train_prec * 100:6.2f}     â”‚ {rf_train_rec * 100:6.2f}     â”‚ {rf_train_f1:.4f}     â”‚")
    print(
        f"    â”‚ é€»è¾‘å›å½’        â”‚ {lr_train_acc * 100:6.2f}     â”‚ {lr_train_prec * 100:6.2f}     â”‚ {lr_train_rec * 100:6.2f}     â”‚ {lr_train_f1:.4f}     â”‚")
    print("    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜")
    print(f"    ğŸ’¡ æ¨èæ¨¡å‹ï¼š{'éšæœºæ£®æ—' if rf_train_f1 > lr_train_f1 else 'é€»è¾‘å›å½’'}ï¼ˆF1åˆ†æ•°æ›´é«˜ï¼‰")

    # ==================== STEP 6: åœ¨ä¸»æ•°æ®é›†ä¸Šé¢„æµ‹ ====================
    print("\n" + "â–¶" * 40)
    print("STEP 6/6: å¯¹ä¸»æ•°æ®é›†è¿›è¡Œé¢„æµ‹")
    print("â–¶" * 40)

    print("[1/3] å‡†å¤‡é¢„æµ‹æ•°æ®...")
    df_pred = df_sens.copy()
    df_pred = df_pred.dropna(subset=feature_cols)
    print(f"    âœ… å‡†å¤‡å®Œæˆï¼š{len(df_pred)} æ¡æœ‰æ•ˆæ•°æ®ï¼ˆç¼ºå¤±ç‰¹å¾å·²å»é™¤ï¼‰")

    print("\n[2/3] ä½¿ç”¨éšæœºæ£®æ—æ¨¡å‹é¢„æµ‹...")
    progress = ProgressBar(len(df_pred), desc="RF Predict")

    X_pred = scaler.transform(df_pred[feature_cols])

    # åˆ†æ‰¹é¢„æµ‹ï¼ˆæ˜¾ç¤ºè¿›åº¦ï¼‰
    batch_size = 5000
    rf_preds = []
    rf_probs = []

    for i in range(0, len(X_pred), batch_size):
        batch_X = X_pred[i:i + batch_size]
        rf_preds.extend(rf_model.predict(batch_X))
        rf_probs.extend(rf_model.predict_proba(batch_X)[:, 1])
        progress.update(len(batch_X))

    progress.finish()

    df_pred['fake_pred_rf'] = rf_preds
    df_pred['fake_prob_rf'] = rf_probs

    print(f"    âœ… é¢„æµ‹å®Œæˆ")
    print(f"       - é¢„æµ‹å‡æ–°é—»æ¯”ä¾‹ï¼š{df_pred['fake_pred_rf'].mean() * 100:.1f}%")
    print(f"       - æ¦‚ç‡èŒƒå›´ï¼š{df_pred['fake_prob_rf'].min():.3f} - {df_pred['fake_prob_rf'].max():.3f}")

    print("\n[3/3] ä½¿ç”¨é€»è¾‘å›å½’æ¨¡å‹é¢„æµ‹...")
    progress = ProgressBar(len(df_pred), desc="LR Predict")

    lr_preds = []
    lr_probs = []

    for i in range(0, len(X_pred), batch_size):
        batch_X = X_pred[i:i + batch_size]
        lr_preds.extend(lr_model.predict(batch_X))
        lr_probs.extend(lr_model.predict_proba(batch_X)[:, 1])
        progress.update(len(batch_X))

    progress.finish()

    df_pred['fake_pred_lr'] = lr_preds
    df_pred['fake_prob_lr'] = lr_probs

    print(f"    âœ… é¢„æµ‹å®Œæˆ")
    print(f"       - é¢„æµ‹å‡æ–°é—»æ¯”ä¾‹ï¼š{df_pred['fake_pred_lr'].mean() * 100:.1f}%")
    print(f"       - æ¦‚ç‡èŒƒå›´ï¼š{df_pred['fake_prob_lr'].min():.3f} - {df_pred['fake_prob_lr'].max():.3f}")

    # ==================== ä¸»é¢˜çº§åˆ†æï¼ˆå¯¹æ•°ç›¸å…³ä¿®æ­£ç‰ˆï¼‰ ====================
    print("\n" + "â–¶" * 40)
    print("ä¸»é¢˜çº§åˆ†æä¸å¯¹æ•°ç›¸å…³è®¡ç®—ï¼ˆä¿®æ­£ç‰ˆï¼‰")
    print("ğŸ” æ ¸å¿ƒï¼šlog(è€¸äººå¬é—»æŒ‡æ•°) vs åŸå§‹å‡æ–°é—»ç‡ï¼ˆ0-1æ¯”ä¾‹ï¼‰")
    print("â–¶" * 40)

    print("[1/3] è®¡ç®—ä¸»é¢˜çº§æŒ‡æ ‡...")
    progress = ProgressBar(df_pred['lda_topic'].nunique(), desc="Topics")

    topic_analysis = df_pred.groupby('lda_topic').agg(
        headline_count=('headline_text', 'count'),
        predicted_fake_ratio_rf=('fake_pred_rf', 'mean'),  # åŸå§‹å‡æ–°é—»ç‡ï¼ˆ0-1ï¼‰
        predicted_fake_ratio_lr=('fake_pred_lr', 'mean'),  # åŸå§‹å‡æ–°é—»ç‡ï¼ˆ0-1ï¼‰
        avg_fake_prob_rf=('fake_prob_rf', 'mean'),
        avg_fake_prob_lr=('fake_prob_lr', 'mean'),
        avg_sensationalism=('sensationalism_score', 'mean'),  # åŸå§‹è€¸äººæŒ‡æ•°
        avg_sentiment=('sentiment_compound', 'mean'),
        avg_similarity=('max_similarity', 'mean')
    ).reset_index()

    # è°ƒæ•´æœ‰æ•ˆä¸»é¢˜çš„æœ€å°æ ·æœ¬æ•°ï¼ˆé€‚åº”1/10é‡‡æ ·ï¼‰
    min_samples_per_topic = max(10, int(20 * sampling_ratio))  # æŒ‰æ¯”ä¾‹è°ƒæ•´æœ€å°æ ·æœ¬æ•°
    topic_analysis = topic_analysis[topic_analysis['headline_count'] >= min_samples_per_topic].reset_index(drop=True)
    progress.update(len(topic_analysis))
    progress.finish()

    print(f"    âœ… å®Œæˆ | å…± {len(topic_analysis)} ä¸ªæœ‰æ•ˆä¸»é¢˜ï¼ˆâ‰¥{min_samples_per_topic}æ¡æ•°æ®ï¼‰")

    print("\n[2/3] å¯¹æ•°å˜æ¢ä¸ç›¸å…³ç³»æ•°è®¡ç®—...")
    print(f"    ğŸ“Œ å¯¹æ•°å˜æ¢è¯´æ˜ï¼šä»…å¯¹è€¸äººå¬é—»æŒ‡æ•°åšå¯¹æ•°å˜æ¢ï¼ˆè§£å†³åˆ†å¸ƒåæ–œï¼‰")
    print(f"    ğŸ“Œ å˜æ¢å…¬å¼ï¼šlog(avg_sensationalism + {log_transform_offset})")
    print(f"    ğŸ“Œ å‡æ–°é—»ç‡ä¿æŒåŸå§‹å€¼ï¼ˆ0-1æ¯”ä¾‹ï¼‰ï¼Œä¾¿äºç›´è§‚è§£è¯»")

    def safe_log_transform(data, offset=1e-3):
        """å®‰å…¨çš„å¯¹æ•°å˜æ¢ï¼šå¤„ç†0å€¼å’Œè´Ÿå€¼"""
        # ç¡®ä¿æ•°æ®ä¸ºæ­£æ•°ï¼ˆè€¸äººæŒ‡æ•°æœ¬èº«æ˜¯0-1ï¼ŒåŠ åç§»é‡åæ›´å®‰å…¨ï¼‰
        data_positive = data + offset
        # å¯¹æ•°å˜æ¢
        return np.log(data_positive)

    # ä»…å¯¹è€¸äººå¬é—»æŒ‡æ•°è¿›è¡Œå¯¹æ•°å˜æ¢ï¼ˆæ ¸å¿ƒä¿®æ”¹ï¼‰
    topic_analysis['log_avg_sensationalism'] = safe_log_transform(
        topic_analysis['avg_sensationalism'],
        offset=log_transform_offset
    )

    # ã€æ ¸å¿ƒä¿®æ”¹ã€‘è®¡ç®— logè€¸äººæŒ‡æ•° vs åŸå§‹å‡æ–°é—»ç‡ çš„ç›¸å…³ç³»æ•°
    # RFæ¨¡å‹ç›¸å…³
    log_corr_rf, log_p_rf = pearsonr(
        topic_analysis['log_avg_sensationalism'],  # logå˜æ¢åçš„è€¸äººæŒ‡æ•°
        topic_analysis['predicted_fake_ratio_rf']   # åŸå§‹å‡æ–°é—»ç‡ï¼ˆ0-1ï¼‰
    )

    # LRæ¨¡å‹ç›¸å…³
    log_corr_lr, log_p_lr = pearsonr(
        topic_analysis['log_avg_sensationalism'],  # logå˜æ¢åçš„è€¸äººæŒ‡æ•°
        topic_analysis['predicted_fake_ratio_lr']   # åŸå§‹å‡æ–°é—»ç‡ï¼ˆ0-1ï¼‰
    )

    print(f"    âœ… å®Œæˆ")
    print(f"\n    ğŸ“Š å¯¹æ•°ç›¸å…³åˆ†æç»“æœï¼ˆä¿®æ­£ç‰ˆï¼‰ï¼š")
    print(f"    ğŸ”‘ åˆ†æç»´åº¦ï¼šlog(è€¸äººå¬é—»æŒ‡æ•°) vs åŸå§‹å‡æ–°é—»ç‡ï¼ˆ0-1æ¯”ä¾‹ï¼‰")
    print(f"    â”œâ”€ éšæœºæ£®æ—æ¨¡å‹ï¼š")
    print(f"    â”‚  â”œâ”€ ç›¸å…³ç³»æ•° r = {log_corr_rf:+.4f}")
    print(f"    â”‚  â”œâ”€ p å€¼ = {log_p_rf:.4f}")
    print(f"    â”‚  â””â”€ ç»“æœï¼š{'âœ… æ˜¾è‘—ç›¸å…³ (p<0.05)' if log_p_rf < 0.05 else 'âŒ ä¸æ˜¾è‘— (pâ‰¥0.05)'}")
    print(f"    â”œâ”€ é€»è¾‘å›å½’æ¨¡å‹ï¼š")
    print(f"    â”‚  â”œâ”€ ç›¸å…³ç³»æ•° r = {log_corr_lr:+.4f}")
    print(f"    â”‚  â”œâ”€ p å€¼ = {log_p_lr:.4f}")
    print(f"    â”‚  â””â”€ ç»“æœï¼š{'âœ… æ˜¾è‘—ç›¸å…³ (p<0.05)' if log_p_lr < 0.05 else 'âŒ ä¸æ˜¾è‘— (pâ‰¥0.05)'}")
    print(f"    â””â”€ ç›¸å…³æ€§å¼ºåº¦è§£è¯»ï¼š")
    if abs(log_corr_rf) > 0.7:
        print(f"       ğŸ¯ å¼ºç›¸å…³ï¼logè€¸äººæŒ‡æ•°ä¸å‡æ–°é—»ç‡é«˜åº¦å…³è”")
    elif abs(log_corr_rf) > 0.5:
        print(f"       âš ï¸  ä¸­ç­‰å¼ºç›¸å…³ï¼Œå…³è”ç¨‹åº¦è¾ƒé«˜")
    elif abs(log_corr_rf) > 0.3:
        print(f"       ğŸ“Š ä¸­ç­‰ç›¸å…³ï¼Œå­˜åœ¨æ˜æ˜¾å…³è”")
    else:
        print(f"       ğŸ“‹ å¼±ç›¸å…³ï¼ˆå¯èƒ½å—1/10é‡‡æ ·å½±å“ï¼‰ï¼Œå»ºè®®ä½¿ç”¨æ›´å¤§é‡‡æ ·æ¯”ä¾‹éªŒè¯")

    # è¾“å‡ºåŸå§‹æ•°æ®å’Œå˜æ¢åæ•°æ®çš„ç»Ÿè®¡ä¿¡æ¯
    print(f"\n    ğŸ“ˆ æ•°æ®ç»Ÿè®¡ï¼š")
    print(
        f"    â”œâ”€ å¹³å‡è€¸äººæŒ‡æ•°ï¼ˆåŸå§‹ï¼‰ï¼š{topic_analysis['avg_sensationalism'].mean():.3f} Â± {topic_analysis['avg_sensationalism'].std():.3f}")
    print(
        f"    â”œâ”€ å¹³å‡è€¸äººæŒ‡æ•°ï¼ˆå¯¹æ•°ï¼‰ï¼š{topic_analysis['log_avg_sensationalism'].mean():.3f} Â± {topic_analysis['log_avg_sensationalism'].std():.3f}")
    print(
        f"    â”œâ”€ å¹³å‡å‡æ–°é—»ç‡ï¼ˆRFï¼ŒåŸå§‹ï¼‰ï¼š{topic_analysis['predicted_fake_ratio_rf'].mean():.3f} Â± {topic_analysis['predicted_fake_ratio_rf'].std():.3f}")
    print(
        f"    â””â”€ å‡æ–°é—»ç‡èŒƒå›´ï¼ˆRFï¼‰ï¼š{topic_analysis['predicted_fake_ratio_rf'].min():.3f} - {topic_analysis['predicted_fake_ratio_rf'].max():.3f}")

    print("\n[3/3] ä¸»é¢˜è¯¦ç»†æ•°æ®...")
    print("\n    ğŸ“‹ ä¸»é¢˜çº§æŒ‡æ ‡è¯¦è¡¨ï¼ˆæŒ‰å‡æ–°é—»æ¯”ä¾‹æ’åºï¼ŒTOP 10ï¼‰ï¼š")
    print("    â”Œâ”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”")
    print("    â”‚ä¸»é¢˜â”‚æ ·æœ¬æ•°â”‚å‡æ–°é—»ç‡(RF)  â”‚å‡æ–°é—»ç‡(LR)  â”‚åŸå§‹è€¸äººæŒ‡æ•°  â”‚logè€¸äººæŒ‡æ•°  â”‚å¹³å‡æƒ…æ„Ÿåˆ†æ•°  â”‚")
    print("    â”œâ”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤")

    for idx, row in topic_analysis.nlargest(10, 'predicted_fake_ratio_rf').iterrows():
        print(f"    â”‚{int(row['lda_topic']):3d} â”‚{int(row['headline_count']):5d} â”‚"
              f"    {row['predicted_fake_ratio_rf'] * 100:5.1f}%    â”‚"
              f"    {row['predicted_fake_ratio_lr'] * 100:5.1f}%    â”‚"
              f"    {row['avg_sensationalism']:5.3f}    â”‚"
              f"  {row['log_avg_sensationalism']:6.3f}  â”‚"
              f"   {row['avg_sentiment']:+6.3f}    â”‚")

    print("    â””â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜")

    # ==================== ä¿å­˜ç»“æœï¼ˆä¿®æ­£ç‰ˆï¼‰ ====================
    print("\n" + "â–¶" * 40)
    print("ä¿å­˜ç»“æœæ–‡ä»¶ï¼ˆä¿®æ­£ç‰ˆï¼‰")
    print("â–¶" * 40)

    print("[1/3] ä¿å­˜é¢„æµ‹ç»“æœ...")
    df_pred[['headline_text', 'lda_topic', 'sensationalism_score', 'sentiment_compound',
             'fake_pred_rf', 'fake_prob_rf', 'fake_pred_lr', 'fake_prob_lr']].to_csv(
        'fake_news_predictions_log_corr_fixed_10pct.csv', index=False, encoding='utf-8'
    )
    print("    âœ… ä¿å­˜ï¼šfake_news_predictions_log_corr_fixed_10pct.csv")

    print("[2/3] ä¿å­˜ä¸»é¢˜çº§åˆ†æï¼ˆä¿®æ­£ç‰ˆï¼‰...")
    # ä¿å­˜åŸå§‹æ•°æ®å’Œlogå˜æ¢åçš„è€¸äººæŒ‡æ•°ï¼ˆä¸ä¿å­˜logå‡æ–°é—»ç‡ï¼‰
    topic_analysis_output = topic_analysis[['lda_topic', 'headline_count', 'predicted_fake_ratio_rf',
                                            'predicted_fake_ratio_lr', 'avg_fake_prob_rf', 'avg_fake_prob_lr',
                                            'avg_sensationalism', 'log_avg_sensationalism',  # åŒ…å«logè€¸äººæŒ‡æ•°
                                            'avg_sentiment', 'avg_similarity']].copy()
    topic_analysis_output.to_csv('topic_analysis_log_corr_fixed_10pct.csv', index=False, encoding='utf-8')
    print("    âœ… ä¿å­˜ï¼štopic_analysis_log_corr_fixed_10pct.csv")

    print("[3/3] ä¿å­˜è®­ç»ƒæ•°æ®...")
    training_df.to_csv('training_data_used_log_corr_fixed_10pct.csv', index=False, encoding='utf-8')
    print("    âœ… ä¿å­˜ï¼štraining_data_used_log_corr_fixed_10pct.csv")

    # ==================== æœ€ç»ˆæ€»ç»“ ====================
    print("\n" + "=" * 80)
    print("ğŸ‰ åˆ†æå®Œæˆï¼æœ€ç»ˆæ€»ç»“ï¼ˆä¿®æ­£ç‰ˆï¼šlogè€¸äººæŒ‡æ•° vs åŸå§‹å‡æ–°é—»ç‡ï¼‰")
    print("=" * 80)

    total_time = train_time_rf + train_time_lr

    print(f"""
âš¡ è¿è¡Œæ•ˆç‡ç»Ÿè®¡ï¼ˆè¶…å¿«é€Ÿæ¨¡å¼ - 1/10æ•°æ®é‡‡æ ·ï¼‰ï¼š
   â€¢ æ ‡æ³¨æ•°æ®é‡‡æ ·æ¯”ä¾‹ï¼š{sampling_ratio * 100:.0f}% (Fake.csvå’ŒTrue.csv)
   â€¢ ä¸»æ•°æ®é›†é‡‡æ ·æ¯”ä¾‹ï¼š{main_data_sampling_ratio * 100:.0f}% (sensationalism_pkl)
   â€¢ ç‰¹å¾è®¡ç®—è€—æ—¶ï¼š~30ç§’ - 2åˆ†é’Ÿï¼ˆæ¯”åŸç‰ˆå¿«10å€ï¼‰
   â€¢ æ¨¡å‹è®­ç»ƒè€—æ—¶ï¼š{total_time:.1f}sï¼ˆæ¯”åŸç‰ˆå¿«10å€ï¼‰
   â€¢ é¢„æµ‹+åˆ†æè€—æ—¶ï¼š~30ç§’ - 1åˆ†é’Ÿï¼ˆæ¯”åŸç‰ˆå¿«10å€ï¼‰
   â€¢ ğŸš€ æ€»ä½“é€Ÿåº¦æå‡ï¼šçº¦10å€ï¼ˆç›¸æ¯”å…¨é‡æ•°æ®ï¼‰

ğŸ“Š æ•°æ®ç»Ÿè®¡ï¼š
   â€¢ ä¸»æ•°æ®é›†ï¼š{len(df_pred):,} æ¡æ•°æ®ï¼ˆåŸå§‹çº¦ {int(len(df_pred) / main_data_sampling_ratio):,} æ¡ï¼‰
   â€¢ æ ‡æ³¨æ•°æ®ï¼š{len(training_df)} æ¡ï¼ˆç”¨äºè®­ç»ƒï¼‰
   â€¢ ä¸»é¢˜æ•°é‡ï¼š{len(topic_analysis)} ä¸ªæœ‰æ•ˆä¸»é¢˜ï¼ˆâ‰¥{min_samples_per_topic}æ¡æ•°æ®ï¼‰

ğŸ”§ æ¨¡å‹æ€§èƒ½ï¼š
   â€¢ éšæœºæ£®æ— - å‡†ç¡®ç‡ {rf_train_acc:.2%}ï¼ŒF1åˆ†æ•° {rf_train_f1:.4f}
   â€¢ é€»è¾‘å›å½’ - å‡†ç¡®ç‡ {lr_train_acc:.2%}ï¼ŒF1åˆ†æ•° {lr_train_f1:.4f}

ğŸ“ˆ æ ¸å¿ƒå‘ç°ï¼ˆä¿®æ­£ç‰ˆå¯¹æ•°ç›¸å…³ï¼‰ï¼š
   â€¢ åˆ†æç»´åº¦ï¼šlog(è€¸äººå¬é—»æŒ‡æ•°) vs åŸå§‹å‡æ–°é—»ç‡ï¼ˆ0-1æ¯”ä¾‹ï¼‰
   â€¢ éšæœºæ£®æ—æ¨¡å‹ï¼šr = {log_corr_rf:+.4f}ï¼Œp = {log_p_rf:.4f}
   â€¢ é€»è¾‘å›å½’æ¨¡å‹ï¼šr = {log_corr_lr:+.4f}ï¼Œp = {log_p_lr:.4f}
   â€¢ é¢„æµ‹å‡æ–°é—»æ¯”ä¾‹ï¼ˆRFï¼‰ï¼š{df_pred['fake_pred_rf'].mean() * 100:.1f}%
   â€¢ é¢„æµ‹å‡æ–°é—»æ¯”ä¾‹ï¼ˆLRï¼‰ï¼š{df_pred['fake_pred_lr'].mean() * 100:.1f}%
   â€¢ å¯¹æ•°å˜æ¢åç§»é‡ï¼š{log_transform_offset}

ğŸ’¾ è¾“å‡ºæ–‡ä»¶ï¼š
   1. fake_news_predictions_log_corr_fixed_10pct.csv - å®Œæ•´é¢„æµ‹ç»“æœï¼ˆ1/10é‡‡æ ·ï¼‰
   2. topic_analysis_log_corr_fixed_10pct.csv - ä¸»é¢˜çº§æŒ‡æ ‡ï¼ˆå«logè€¸äººæŒ‡æ•°ï¼‰
   3. training_data_used_log_corr_fixed_10pct.csv - è®­ç»ƒæ•°æ®ï¼ˆ1/10é‡‡æ ·ï¼‰

âœ¨ å…³é”®è¯´æ˜ï¼š
   â€¢ å‡æ–°é—»ç‡ä¿æŒåŸå§‹å€¼ï¼ˆ0-1ï¼‰ï¼Œè¡¨ç¤ºè¯¥ä¸»é¢˜ä¸­é¢„æµ‹ä¸ºå‡æ–°é—»çš„æ¯”ä¾‹ï¼Œç›´è§‚æ˜“è§£è¯»
   â€¢ ä»…å¯¹è€¸äººå¬é—»æŒ‡æ•°åšå¯¹æ•°å˜æ¢ï¼Œè§£å†³å…¶å¯èƒ½çš„åˆ†å¸ƒåæ–œé—®é¢˜
   â€¢ ç›¸å…³ç³»æ•°åæ˜ äº†ã€Œå¯¹æ•°è€¸äººæŒ‡æ•°ã€ä¸ã€Œå‡æ–°é—»æ¯”ä¾‹ã€çš„çº¿æ€§å…³è”å¼ºåº¦
   â€¢ å¦‚éœ€æé«˜å‡†ç¡®æ€§ï¼Œå¯è°ƒæ•´å‚æ•°ï¼šsampling_ratio=0.5 æˆ– 1.0ï¼ˆå…¨é‡æ•°æ®ï¼‰
    """)

    print("=" * 80)
    print("æ„Ÿè°¢ä½¿ç”¨ï¼ğŸ“§ å¦‚æœ‰é—®é¢˜ï¼Œè¯·æ£€æŸ¥è¾“å‡ºæ–‡ä»¶ä¸­çš„è¯¦ç»†æ•°æ®")
    print("=" * 80 + "\n")

    return df_pred, topic_analysis, training_df


# ===================== æ‰§è¡Œå…¥å£ =====================
if __name__ == "__main__":
    # ã€è¶…å¿«é€Ÿæµ‹è¯•ç‰ˆæœ¬ã€‘ï¼šé»˜è®¤ä½¿ç”¨1/10æ•°æ®ï¼ˆsampling_ratio=0.1ï¼‰
    print("\n" + "âš¡" * 40)
    print("ğŸ”¥ è¶…å¿«é€Ÿæµ‹è¯•æ¨¡å¼å¯åŠ¨ï¼ˆä»…ä½¿ç”¨1/10æ•°æ®ï¼Œé€Ÿåº¦å¿«10å€ï¼‰")
    print("ğŸ” ä¿®æ­£ç‰ˆå¯¹æ•°ç›¸å…³ï¼šlogè€¸äººå¬é—»æŒ‡æ•° vs åŸå§‹å‡æ–°é—»ç‡")
    print("âš¡" * 40)

    df_pred, topic_analysis, training_df = improved_fake_news_detection_ml_based(
        sensationalism_pkl='full_data_with_sensationalism_score.pkl',
        fake_news_csv='Fake.csv',
        real_news_csv='True.csv',
        threshold=65,
        sample_size=None,  # None = ä½¿ç”¨å…¨éƒ¨é‡‡æ ·åçš„æ•°æ®
        sampling_ratio=0.1,  # æ ‡æ³¨æ•°æ®1/10é‡‡æ ·ï¼ˆå…³é”®å‚æ•°ï¼‰
        main_data_sampling_ratio=0.1,  # ä¸»æ•°æ®é›†1/10é‡‡æ ·ï¼ˆå…³é”®å‚æ•°ï¼‰
        log_transform_offset=1e-3  # å¯¹æ•°å˜æ¢åç§»é‡ï¼Œå¯æ ¹æ®æ•°æ®è°ƒæ•´
    )

    # ã€è°ƒæ•´é‡‡æ ·æ¯”ä¾‹ç¤ºä¾‹ã€‘ï¼šå¦‚éœ€ä½¿ç”¨1/5æ•°æ®ï¼Œå–æ¶ˆä¸‹é¢æ³¨é‡Š
    # df_pred, topic_analysis, training_df = improved_fake_news_detection_ml_based(
    #     sensationalism_pkl='full_data_with_sensationalism_score.pkl',
    #     fake_news_csv='Fake.csv',
    #     real_news_csv='True.csv',
    #     threshold=65,
    #     sample_size=None,
    #     sampling_ratio=0.2,  # 1/5æ•°æ®
    #     main_data_sampling_ratio=0.2,  # ä¸»æ•°æ®é›†ä¹Ÿ1/5é‡‡æ ·
    #     log_transform_offset=1e-3
    # )

    # ã€ä½¿ç”¨å®Œæ•´æ•°æ®ã€‘ï¼šå–æ¶ˆä¸‹é¢æ³¨é‡Šï¼Œæ”¹ä¸ºé‡‡æ ·æ¯”ä¾‹1.0
    # df_pred, topic_analysis, training_df = improved_fake_news_detection_ml_based(
    #     sensationalism_pkl='full_data_with_sensationalism_score.pkl',
    #     fake_news_csv='Fake.csv',
    #     real_news_csv='True.csv',
    #     threshold=65,
    #     sample_size=None,
    #     sampling_ratio=1.0,  # å®Œæ•´æ•°æ®
    #     main_data_sampling_ratio=1.0,  # å®Œæ•´ä¸»æ•°æ®é›†
    #     log_transform_offset=1e-3
    # )