# @title Advanced Analysis: RoBERTa Sentiment, Anomaly Detection, Sensationalism Scoring
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sb
from scipy.stats import pearsonr
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report
from sklearn.preprocessing import MinMaxScaler
from collections import Counter
import re
import joblib

# è®¾ç½®å¯è§†åŒ–æ ·å¼ï¼ˆç§»é™¤ä¸­æ–‡ä¾èµ–ï¼Œå…¨éƒ¨ä½¿ç”¨è‹±æ–‡ï¼‰
plt.rcParams['axes.unicode_minus'] = False
plt.style.use('seaborn-v0_8-whitegrid')

# ===================== åŠ è½½åŸºç¡€æ•°æ®å’Œæ¨¡å‹ =====================
# åŠ è½½æ•°æ®é›†ï¼ˆä½ çš„æ•°æ®æœ‰ 49,831 æ¡ï¼Œæ˜¯å­é›†ï¼Œä¸å½±å“ï¼‰
df = pd.read_pickle('final_data_with_topic_sentiment.pkl')
print(f"âœ… åŸºç¡€æ•°æ®é›†åŠ è½½æˆåŠŸï¼Œå…± {len(df):,} æ¡æ•°æ®")


# ---------------------- å¼ºåˆ¶ç”Ÿæˆ sentiment_polarity åˆ—ï¼ˆå…³é”®ä¿®å¤ï¼‰----------------------
# ç›´æ¥æ ¹æ® sentiment_compound è®¡ç®—ï¼Œè¦†ç›–æˆ–æ–°å¢è¯¥åˆ—
def get_sentiment_polarity(compound):
    if compound > 0.05:
        return 'Positive'  # æ”¹ä¸ºè‹±æ–‡ï¼Œé¿å…åç»­å¤„ç†ä¸­æ–‡
    elif compound < -0.05:
        return 'Negative'
    else:
        return 'Neutral'


# å¼ºåˆ¶ç”Ÿæˆåˆ—ï¼ˆä¸ç®¡ä¹‹å‰æœ‰æ²¡æœ‰ï¼‰
df['sentiment_polarity'] = df['sentiment_compound'].apply(get_sentiment_polarity)

# æ‰“å°åˆ—åç¡®è®¤ï¼Œè®©ä½ çœ‹åˆ°è¯¥åˆ—å·²å­˜åœ¨
print(f"âœ… æ•°æ®é›†å½“å‰åˆ—åï¼š{df.columns.tolist()}")
print(f"âœ… sentiment_polarity åˆ—ç”ŸæˆæˆåŠŸï¼Œåˆ†å¸ƒï¼š\n{df['sentiment_polarity'].value_counts()}")

# åŠ è½½ LDA æ¨¡å‹ï¼ˆç”¨äºä¸»é¢˜åŒ¹é…ï¼‰
try:
    lda = joblib.load('lda_topic_model.pkl')
    vectorizer = joblib.load('tfidf_vectorizer.pkl')
    print("âœ… LDA æ¨¡å‹å’Œ TF-IDF å‘é‡å™¨åŠ è½½æˆåŠŸ")
except FileNotFoundError as e:
    print(f"âš ï¸  æœªæ‰¾åˆ°æ¨¡å‹æ–‡ä»¶ï¼š{e}ï¼Œä¸»é¢˜ç›¸å…³åŠŸèƒ½å¯èƒ½å—å½±å“")


# ===================== 1. Advanced Sentiment Model: VADER vs RoBERTa =====================
def generate_annotation_template(n_samples=150):
    """
    ç”Ÿæˆæ‰‹åŠ¨æ ‡æ³¨æ¨¡æ¿ï¼ˆCSVæ–‡ä»¶ï¼‰ï¼Œç”¨äºå¯¹æ¯” VADER å’Œ RoBERTa å‡†ç¡®æ€§
    éšæœºæŠ½å– 150 æ¡æ•°æ®ï¼Œæ¶µç›–ä¸åŒä¸»é¢˜å’Œ VADER æƒ…æ„Ÿææ€§
    """
    try:
        # åˆ†å±‚æŠ½æ ·ï¼šæ·»åŠ  group_keys=False æ¶ˆé™¤ DeprecationWarning
        sample_df = df.groupby(['lda_topic', 'sentiment_polarity'], dropna=True, group_keys=False).apply(
            lambda x: x.sample(min(5, len(x)), random_state=42)
        ).reset_index(drop=True)
    except:
        # è‹¥åˆ†å±‚æŠ½æ ·å¤±è´¥ï¼ˆæ¯”å¦‚æŸäº›ä¸»é¢˜+æƒ…æ„Ÿç»„åˆä¸ºç©ºï¼‰ï¼Œæ”¹ç”¨ç®€å•éšæœºæŠ½æ ·ï¼ˆç¡®ä¿èƒ½ç”Ÿæˆæ¨¡æ¿ï¼‰
        print("âš ï¸  åˆ†å±‚æŠ½æ ·å¤±è´¥ï¼Œæ”¹ç”¨ç®€å•éšæœºæŠ½æ ·ç”Ÿæˆæ¨¡æ¿")
        sample_df = df.sample(min(n_samples, len(df)), random_state=42).reset_index(drop=True)

    # è¡¥å……åˆ°ç›®æ ‡æ ·æœ¬æ•°
    if len(sample_df) < n_samples:
        remaining = n_samples - len(sample_df)
        è¡¥å……_samples = df.drop(sample_df.index).sample(remaining, random_state=42)
        sample_df = pd.concat([sample_df, è¡¥å……_samples], ignore_index=True)

    # ç”Ÿæˆæ ‡æ³¨æ¨¡æ¿ï¼ˆå­—æ®µåæ”¹ä¸ºè‹±æ–‡ï¼Œæ–¹ä¾¿æ ‡æ³¨ï¼‰
    annotation_template = sample_df[['headline_text', 'lda_topic', 'sentiment_compound', 'sentiment_polarity']].copy()
    annotation_template['manual_sentiment'] = ''  # å¡«å†™ï¼š1=Positive, 0=Neutral, -1=Negative
    annotation_template['notes'] = ''  # å¯é€‰ï¼šè®°å½•è®½åˆºã€æ··åˆæƒ…æ„Ÿï¼ˆè‹±æ–‡å¤‡æ³¨ï¼‰

    annotation_template.to_csv('sentiment_annotation_template.csv', index=False, encoding='utf-8')
    print(f"ğŸ“‹ æ‰‹åŠ¨æ ‡æ³¨æ¨¡æ¿å·²ç”Ÿæˆï¼šsentiment_annotation_template.csv")
    print(f"â• æ“ä½œè¯´æ˜ï¼šæ‰“å¼€ CSV æ–‡ä»¶ï¼Œåœ¨ manual_sentiment åˆ—å¡«å†™çœŸå®æƒ…æ„Ÿï¼ˆ1=Positive, 0=Neutral, -1=Negativeï¼‰")


def compare_vader_roberta(annotated_csv='sentiment_annotation_template.csv'):
    """
    å¯¹æ¯” VADER å’Œ RoBERTa çš„æƒ…æ„Ÿé¢„æµ‹å‡†ç¡®æ€§
    è¾“å…¥ï¼šå¡«å†™å®Œæˆçš„æ‰‹åŠ¨æ ‡æ³¨ CSV æ–‡ä»¶
    è¾“å‡ºï¼šå‡†ç¡®ç‡å¯¹æ¯”ã€æ··æ·†çŸ©é˜µã€ç»†å¾®æƒ…æ„Ÿæ•æ‰æ¡ˆä¾‹
    """
    # åŠ è½½æ‰‹åŠ¨æ ‡æ³¨æ•°æ®
    try:
        annotated_df = pd.read_csv(annotated_csv)
        # è¿‡æ»¤æœªæ ‡æ³¨æ•°æ®å’Œæ— æ•ˆæ ‡æ³¨ï¼ˆä»…ä¿ç•™ 1/0/-1ï¼‰
        annotated_df = annotated_df[
            (annotated_df['manual_sentiment'].notna()) &
            (annotated_df['manual_sentiment'].isin([1, 0, -1]))
            ].reset_index(drop=True)
        print(f"âœ… åŠ è½½æ ‡æ³¨æ•°æ®æˆåŠŸï¼Œå…± {len(annotated_df)} æ¡æœ‰æ•ˆæ ‡æ³¨")
    except FileNotFoundError:
        print(f"âŒ æœªæ‰¾åˆ°æ ‡æ³¨æ–‡ä»¶ï¼š{annotated_csv}")
        print("è¯·ç¡®è®¤æ–‡ä»¶è·¯å¾„æ­£ç¡®ï¼Œä¸”å·²å¡«å†™ manual_sentiment åˆ—")
        return

    # ---------------------- VADER é¢„æµ‹ç»“æœï¼ˆåŸºäºå·²æœ‰çš„ compound åˆ†æ•°ï¼‰----------------------
    def vader_to_label(compound):
        """å°† VADER çš„ compound åˆ†æ•°è½¬æ¢ä¸º 1/0/-1 æ ‡ç­¾ï¼ˆä¸æ‰‹åŠ¨æ ‡æ³¨ä¸€è‡´ï¼‰"""
        if compound > 0.05:
            return 1
        elif compound < -0.05:
            return -1
        else:
            return 0

    annotated_df['vader_label'] = annotated_df['sentiment_compound'].apply(vader_to_label)

    # ---------------------- ä¸‰åˆ†ç±» RoBERTa æ¨¡å‹ï¼ˆå…³é”®ä¿®æ”¹ï¼šæ”¯æŒ Positive/Neutral/Negativeï¼‰----------------------
    from transformers import pipeline, AutoTokenizer, AutoModelForSequenceClassification
    import torch

    # è®¾ç½®è®¾å¤‡ï¼ˆä¼˜å…ˆ GPUï¼Œæ— åˆ™ç”¨ CPUï¼›GPU è¿è¡Œ 150 æ¡æ ·æœ¬çº¦ 1-2 åˆ†é’Ÿï¼‰
    device = 0 if torch.cuda.is_available() else -1
    print(f"âš™ï¸ RoBERTa è¿è¡Œè®¾å¤‡ï¼š{'GPU' if device == 0 else 'CPU'}ï¼ˆä¸‰åˆ†ç±»æ¨¡å‹ï¼ŒGPU å¯æé€Ÿ 10x+ï¼‰")

    # ä¸‰åˆ†ç±»é¢„è®­ç»ƒæ¨¡å‹ï¼ˆä¸“é—¨é€‚é…æƒ…æ„Ÿä¸‰åˆ†ç±»ï¼Œæ— éœ€è¿‘ä¼¼è®¡ç®—ä¸­æ€§æ¦‚ç‡ï¼‰
    model_name = "cardiffnlp/twitter-roberta-base-sentiment-latest"  # çº¦ 470MBï¼Œé¦–æ¬¡è¿è¡Œè‡ªåŠ¨ä¸‹è½½
    try:
        tokenizer = AutoTokenizer.from_pretrained(model_name)
        roberta_model = pipeline(
            "sentiment-analysis",
            model=model_name,
            tokenizer=tokenizer,
            device=device,
            top_k=None  # æ›¿ä»£ deprecated çš„ return_all_scores=Trueï¼Œæ¶ˆé™¤è­¦å‘Š
        )
        print("âœ… ä¸‰åˆ†ç±» RoBERTa æ¨¡å‹åŠ è½½æˆåŠŸ")
    except Exception as e:
        print(f"âŒ RoBERTa æ¨¡å‹åŠ è½½å¤±è´¥ï¼š{e}")
        print("è§£å†³æ–¹æ¡ˆï¼š1. æ£€æŸ¥ç½‘ç»œè¿æ¥ï¼ˆé¦–æ¬¡ä¸‹è½½çº¦ 470MBï¼‰ï¼›2. å‡çº§ transformers åº“ï¼špip install -U transformers")
        return

    # ä¿®æ­£ RoBERTa é¢„æµ‹é€»è¾‘ï¼ˆé€‚é…ä¸‰åˆ†ç±»è¾“å‡ºï¼Œç›´æ¥è·å–æ¨¡å‹çš„ä¸­æ€§æ ‡ç­¾ï¼‰
    def roberta_predict(text):
        results = roberta_model(text)[0]
        # æ„å»ºæ ‡ç­¾-æ¦‚ç‡æ˜ å°„ï¼ˆé€‚é…æ¨¡å‹çš„å°å†™æ ‡ç­¾è¾“å‡ºï¼‰
        score_dict = {}
        for res in results:
            label = res['label'].strip()  # å…³é”®ä¿®æ”¹ï¼šå–æ¶ˆ .upper()ï¼Œä¿ç•™åŸå§‹å°å†™æ ‡ç­¾
            score_dict[label] = res['score']

        # å–æ¦‚ç‡æœ€å¤§çš„æ ‡ç­¾è½¬æ¢ä¸º 1/0/-1ï¼ˆåŒ¹é…å°å†™æ ‡ç­¾ï¼‰
        max_label = max(score_dict, key=score_dict.get)
        if max_label == 'positive':  # å°å†™æ ‡ç­¾
            return 1, score_dict['positive']
        elif max_label == 'neutral':  # å°å†™æ ‡ç­¾
            return 0, score_dict['neutral']
        elif max_label == 'negative':  # å°å†™æ ‡ç­¾
            return -1, score_dict['negative']
        else:
            # å¼‚å¸¸æƒ…å†µé»˜è®¤ä¸­æ€§
            return 0, 0.0

    # æ‰¹é‡é¢„æµ‹ï¼ˆé¿å…é‡å¤è°ƒç”¨æ¨¡å‹ï¼Œæå‡æ•ˆç‡ï¼‰
    print("ğŸ”„ æ­£åœ¨ç”¨ RoBERTa é¢„æµ‹æƒ…æ„Ÿ...ï¼ˆ150 æ¡æ ·æœ¬çº¦ 1-2 åˆ†é’Ÿï¼ŒGPU æ›´å¿«ï¼‰")
    roberta_labels = []
    roberta_confidences = []
    for text in annotated_df['headline_text'].tolist():
        label, conf = roberta_predict(text)
        roberta_labels.append(label)
        roberta_confidences.append(conf)

    annotated_df['roberta_label'] = roberta_labels
    annotated_df['roberta_confidence'] = roberta_confidences

    # ---------------------- æ¨¡å‹æ€§èƒ½å¯¹æ¯”ï¼ˆæ ¸å¿ƒç»“æœï¼‰----------------------
    # è®¡ç®—å‡†ç¡®ç‡
    vader_acc = accuracy_score(annotated_df['manual_sentiment'], annotated_df['vader_label'])
    roberta_acc = accuracy_score(annotated_df['manual_sentiment'], annotated_df['roberta_label'])

    print("\n" + "=" * 50)
    print("=== VADER vs RoBERTa æƒ…æ„Ÿé¢„æµ‹å‡†ç¡®ç‡å¯¹æ¯” ===")
    print(f"VADER å‡†ç¡®ç‡ï¼š{vader_acc:.2%}")
    print(f"RoBERTa å‡†ç¡®ç‡ï¼š{roberta_acc:.2%}")
    print(f"RoBERTa ç›¸å¯¹æå‡ï¼š{((roberta_acc - vader_acc) / vader_acc * 100):.1f}%")
    print("=" * 50)

    # ç”Ÿæˆåˆ†ç±»æŠ¥å‘Šï¼ˆæ·»åŠ  zero_division=0 æ¶ˆé™¤æœªé¢„æµ‹ç±»åˆ«çš„è­¦å‘Šï¼‰
    print("\n=== VADER åˆ†ç±»æŠ¥å‘Š ===")
    print(classification_report(
        annotated_df['manual_sentiment'],
        annotated_df['vader_label'],
        target_names=['Negative (-1)', 'Neutral (0)', 'Positive (1)'],
        zero_division=0  # æ¶ˆé™¤æœªé¢„æµ‹ç±»åˆ«çš„è­¦å‘Š
    ))

    print("=== RoBERTa åˆ†ç±»æŠ¥å‘Š ===")
    print(classification_report(
        annotated_df['manual_sentiment'],
        annotated_df['roberta_label'],
        target_names=['Negative (-1)', 'Neutral (0)', 'Positive (1)'],
        zero_division=0  # æ¶ˆé™¤æœªé¢„æµ‹ç±»åˆ«çš„è­¦å‘Š
    ))

    # ---------------------- æ··æ·†çŸ©é˜µå¯è§†åŒ–ï¼ˆå­¦æœ¯æŠ¥å‘Šå¿…å¤‡å›¾ï¼‰----------------------
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))

    # VADER æ··æ·†çŸ©é˜µ
    vader_cm = confusion_matrix(annotated_df['manual_sentiment'], annotated_df['vader_label'])
    sb.heatmap(
        vader_cm, annot=True, fmt='d', cmap='Blues', ax=ax1,
        xticklabels=['Negative (-1)', 'Neutral (0)', 'Positive (1)'],
        yticklabels=['Negative (-1)', 'Neutral (0)', 'Positive (1)']
    )
    ax1.set_title(f'VADER Confusion Matrix (Accuracy: {vader_acc:.2%})', fontsize=12, fontweight='bold')
    ax1.set_xlabel('Predicted Label', fontsize=10)
    ax1.set_ylabel('True Label', fontsize=10)

    # RoBERTa æ··æ·†çŸ©é˜µ
    roberta_cm = confusion_matrix(annotated_df['manual_sentiment'], annotated_df['roberta_label'])
    sb.heatmap(
        roberta_cm, annot=True, fmt='d', cmap='Greens', ax=ax2,
        xticklabels=['Negative (-1)', 'Neutral (0)', 'Positive (1)'],
        yticklabels=['Negative (-1)', 'Neutral (0)', 'Positive (1)']
    )
    ax2.set_title(f'RoBERTa Confusion Matrix (Accuracy: {roberta_acc:.2%})', fontsize=12, fontweight='bold')
    ax2.set_xlabel('Predicted Label', fontsize=10)
    ax2.set_ylabel('True Label', fontsize=10)

    plt.tight_layout()
    plt.savefig('vader_roberta_confusion_matrix.png', dpi=150, bbox_inches='tight')
    print("\nğŸ“Š æ··æ·†çŸ©é˜µå›¾å·²ä¿å­˜ï¼švader_roberta_confusion_matrix.pngï¼ˆç›´æ¥æ’å…¥å­¦æœ¯æŠ¥å‘Šï¼‰")

    # ---------------------- ç»†å¾®æƒ…æ„Ÿæ•æ‰æ¡ˆä¾‹ï¼ˆä½“ç°åˆ›æ–°æ€§çš„å…³é”®ï¼‰----------------------
    # ç­›é€‰ RoBERTa æ­£ç¡®ä½† VADER é”™è¯¯çš„æ¡ˆä¾‹ï¼ˆé‡ç‚¹åˆ†æè®½åˆºã€æ··åˆæƒ…æ„Ÿï¼‰
    correct_roberta_incorrect_vader = annotated_df[
        (annotated_df['roberta_label'] == annotated_df['manual_sentiment']) &
        (annotated_df['vader_label'] != annotated_df['manual_sentiment'])
        ].head(8)  # å–å‰ 8 ä¸ªå…¸å‹æ¡ˆä¾‹

    print("\n" + "=" * 60)
    print("=== RoBERTa æ•æ‰ç»†å¾®æƒ…æ„Ÿçš„å…¸å‹æ¡ˆä¾‹ï¼ˆVADER è¯¯åˆ¤ï¼‰===")
    print("æ³¨ï¼šè¿™äº›æ¡ˆä¾‹ä½“ç°äº†é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹å¯¹è¯­ä¹‰çš„æ·±å±‚ç†è§£ä¼˜åŠ¿")
    print("=" * 60)

    for idx, row in correct_roberta_incorrect_vader.iterrows():
        sentiment_map = {-1: 'Negative', 0: 'Neutral', 1: 'Positive'}
        print(f"\nğŸ“ æ–°é—»æ ‡é¢˜ï¼š{row['headline_text']}")
        print(f"ğŸ“Œ çœŸå®æƒ…æ„Ÿï¼ˆæ‰‹åŠ¨æ ‡æ³¨ï¼‰ï¼š{row['manual_sentiment']}ï¼ˆ{sentiment_map[row['manual_sentiment']]}ï¼‰")
        print(f"âŒ VADER é¢„æµ‹ï¼š{row['vader_label']}ï¼ˆCompound åˆ†æ•°ï¼š{row['sentiment_compound']:.3f}ï¼‰")
        print(f"âœ… RoBERTa é¢„æµ‹ï¼š{row['roberta_label']}ï¼ˆç½®ä¿¡åº¦ï¼š{row['roberta_confidence']:.3f}ï¼‰")
        # æ™ºèƒ½åˆ¤æ–­æƒ…æ„Ÿç±»å‹
        text_lower = row['headline_text'].lower()
        if 'won' in text_lower and ('lose' in text_lower or 'defeat' in text_lower):
            reason = 'è®½åˆºï¼ˆSarcasmï¼‰'
        elif 'but' in text_lower or 'however' in text_lower:
            reason = 'æ··åˆæƒ…æ„Ÿï¼ˆMixed Sentimentï¼‰'
        elif row['manual_sentiment'] == 0 and (row['vader_label'] == 1 or row['vader_label'] == -1):
            reason = 'ä¸­æ€§è¯†åˆ«ï¼ˆNeutral Recognitionï¼‰'
        else:
            reason = 'è¯­å¢ƒä¾èµ–ï¼ˆContext-Dependentï¼‰'
        print(f"ğŸ’¡ åŸå› åˆ†æï¼š{reason}")

    # ---------------------- ä¿å­˜å®Œæ•´ç»“æœï¼ˆæ–¹ä¾¿åç»­å¼•ç”¨ï¼‰----------------------
    annotated_df.to_csv('vader_roberta_comparison_results.csv', index=False, encoding='utf-8')
    print("\nğŸ“„ å®Œæ•´å¯¹æ¯”ç»“æœå·²ä¿å­˜ï¼švader_roberta_comparison_results.csvï¼ˆåŒ…å«æ‰€æœ‰é¢„æµ‹æ ‡ç­¾å’Œç½®ä¿¡åº¦ï¼‰")


# ===================== 2. Topicâ€“Sentiment Anomaly Detection =====================
def topic_sentiment_anomaly_detection():
    """ä¸»é¢˜-æƒ…æ„Ÿå¼‚å¸¸æ£€æµ‹ï¼ˆæ— éœ€ä¿®æ”¹ï¼Œå·²é€‚é…ä½ çš„åˆ—ï¼‰"""
    # æŒ‰ä¸»é¢˜è®¡ç®—æƒ…æ„Ÿç»Ÿè®¡é‡
    topic_sentiment_stats = df.groupby('lda_topic')['sentiment_compound'].agg(['mean', 'std']).reset_index()
    topic_sentiment_stats.columns = ['lda_topic', 'topic_sentiment_mean', 'topic_sentiment_std']

    df_with_stats = df.merge(topic_sentiment_stats, on='lda_topic', how='left')
    df_with_stats['z_score'] = (df_with_stats['sentiment_compound'] - df_with_stats['topic_sentiment_mean']) / \
                               df_with_stats['topic_sentiment_std'].replace(0, 0.001)  # é¿å…é™¤é›¶
    df_with_stats['is_anomaly'] = abs(df_with_stats['z_score']) > 2

    # ç»Ÿè®¡ç»“æœ
    anomaly_count = df_with_stats['is_anomaly'].sum()
    anomaly_ratio = anomaly_count / len(df_with_stats)
    print(f"\n=== ä¸»é¢˜-æƒ…æ„Ÿå¼‚å¸¸æ£€æµ‹ç»“æœ ===")
    print(f"å¼‚å¸¸å€¼æ€»æ•°ï¼š{anomaly_count:,} æ¡")
    print(f"å¼‚å¸¸å€¼æ¯”ä¾‹ï¼š{anomaly_ratio:.2%}")

    # å¯è§†åŒ–ï¼ˆæ‰€æœ‰ä¸­æ–‡æ”¹ä¸ºè‹±æ–‡ï¼Œæ¶ˆé™¤å­—ä½“è­¦å‘Šï¼‰
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6))
    anomaly_by_topic = df_with_stats[df_with_stats['is_anomaly']].groupby('lda_topic').size().sort_values(
        ascending=False)
    anomaly_by_topic.plot(kind='bar', color='orange', ax=ax1)
    ax1.set_title('Number of Sentiment Anomalies by Topic', fontsize=12, fontweight='bold')
    ax1.set_xlabel('Topic ID', fontsize=10)
    ax1.set_ylabel('Number of Anomalies', fontsize=10)
    ax1.tick_params(axis='x', rotation=0)

    ax2.hist(
        df_with_stats[~df_with_stats['is_anomaly']]['sentiment_compound'],
        bins=50, alpha=0.5, label='Normal', color='blue', density=True
    )
    ax2.hist(
        df_with_stats[df_with_stats['is_anomaly']]['sentiment_compound'],
        bins=50, alpha=0.7, label='Anomalous', color='red', density=True
    )
    ax2.set_title('Sentiment Score Distribution: Normal vs Anomalous', fontsize=12, fontweight='bold')
    ax2.set_xlabel('Sentiment Compound Score', fontsize=10)
    ax2.set_ylabel('Density', fontsize=10)
    ax2.legend()

    plt.tight_layout()
    plt.savefig('topic_sentiment_anomaly_distribution.png', dpi=150, bbox_inches='tight')
    print("ğŸ“Š å¼‚å¸¸å€¼åˆ†å¸ƒå¯è§†åŒ–å·²ä¿å­˜")

    # ä¿å­˜ç»“æœ
    df_with_stats.to_pickle('full_data_with_anomaly_detection.pkl')
    df_with_stats[['headline_text', 'lda_topic', 'sentiment_compound', 'z_score', 'is_anomaly']].to_csv(
        'sentiment_anomaly_results.csv', index=False, encoding='utf-8'
    )


# ===================== 3. Sensationalism Scoring =====================
def calculate_sensationalism_score():
    """è€¸äººå¬é—»ç¨‹åº¦è¯„åˆ†ï¼ˆæ— éœ€ä¿®æ”¹ï¼Œå·²é€‚é…ä½ çš„åˆ—ï¼‰"""
    df_sens = df.copy()

    # ç‰¹å¾1ï¼šæç«¯æƒ…æ„Ÿ
    df_sens['extreme_sentiment'] = df_sens['sentiment_compound'].apply(lambda x: x*x)#1 if abs(x) > 0.5 else 0)

    # ç‰¹å¾2ï¼šå…¨å¤§å†™è¯æ¯”ä¾‹
    def uppercase_ratio(text):
        words = text.split()
        if len(words) == 0:
            return 0
        uppercase_words = [word for word in words if word.isupper() and len(word) > 1]
        return len(uppercase_words) / len(words)

    df_sens['uppercase_ratio'] = df_sens['headline_text'].apply(uppercase_ratio)

    # ç‰¹å¾3ï¼šæ ‡ç‚¹è®¡æ•°
    df_sens['exclamation_count'] = df_sens['headline_text'].apply(lambda x: x.count('!'))
    df_sens['question_count'] = df_sens['headline_text'].apply(lambda x: x.count('?'))
    max_punct = df_sens[['exclamation_count', 'question_count']].max().max()
    df_sens['punctuation_score'] = (df_sens['exclamation_count'] + df_sens['question_count']) / (max_punct + 1)

    # ç‰¹å¾4ï¼šç‚¹å‡»è¯±é¥µçŸ­è¯­
    clickbait_phrases = [
        "you won't believe", "shocking", "at risk", "breaking", "exclusive",
        "must see", "never before", "secret", "revealed", "how to",
        "this is why", "what happened next", "unbelievable", "terrifying",
        "urgent", "alert", "don't miss", "viral", "explosive"
    ]

    def clickbait_match(text):
        text_lower = text.lower()
        match_count = sum(1 for phrase in clickbait_phrases if phrase in text_lower)
        return min(match_count / 3, 1)

    df_sens['clickbait_score'] = df_sens['headline_text'].apply(clickbait_match)

    # è®¡ç®—æœ€ç»ˆåˆ†æ•°
    weights = {'extreme_sentiment': 1, 'uppercase_ratio': 0, 'punctuation_score': 0, 'clickbait_score': 0}
    df_sens['sensationalism_score'] = (
            df_sens['extreme_sentiment'] * weights['extreme_sentiment'] +
            df_sens['uppercase_ratio'] * weights['uppercase_ratio'] +
            df_sens['punctuation_score'] * weights['punctuation_score'] +
            df_sens['clickbait_score'] * weights['clickbait_score']
    )

    # å½’ä¸€åŒ–
    scaler = MinMaxScaler()
    df_sens['sensationalism_score'] = scaler.fit_transform(df_sens[['sensationalism_score']]).flatten()

    # ç»“æœåˆ†æ
    print(f"\n=== è€¸äººå¬é—»ç¨‹åº¦è¯„åˆ†ç»“æœ ===")
    print(f"åˆ†æ•°èŒƒå›´ï¼š{df_sens['sensationalism_score'].min():.3f} - {df_sens['sensationalism_score'].max():.3f}")
    print(f"å¹³å‡åˆ†æ•°ï¼š{df_sens['sensationalism_score'].mean():.3f}")

    # å¯è§†åŒ–ï¼ˆæ‰€æœ‰ä¸­æ–‡æ”¹ä¸ºè‹±æ–‡ï¼Œæ¶ˆé™¤å­—ä½“è­¦å‘Šï¼‰
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6))
    ax1.hist(df_sens['sensationalism_score'], bins=50, color='purple', alpha=0.7)
    ax1.set_title('Sensationalism Score Distribution', fontsize=12, fontweight='bold')
    ax1.set_xlabel('Score (0=Low Sensationalism, 1=High)', fontsize=10)
    ax1.set_ylabel('Frequency', fontsize=10)

    topic_sens_score = df_sens.groupby('lda_topic')['sensationalism_score'].mean().sort_values(ascending=False)
    topic_sens_score.plot(kind='bar', color='darkred', ax=ax2)
    ax2.set_title('Average Sensationalism Score by Topic', fontsize=12, fontweight='bold')
    ax2.set_xlabel('Topic ID', fontsize=10)
    ax2.set_ylabel('Average Score', fontsize=10)
    ax2.tick_params(axis='x', rotation=0)

    plt.tight_layout()
    plt.savefig('sensationalism_scoring_results.png', dpi=150, bbox_inches='tight')
    print("ğŸ“Š è€¸äººå¬é—»åˆ†æ•°å¯è§†åŒ–å·²ä¿å­˜")

    # ä¿å­˜ç»“æœ
    df_sens.to_pickle('full_data_with_sensationalism_score.pkl')
    df_sens[['headline_text', 'lda_topic', 'sentiment_compound', 'sensationalism_score']].to_csv(
        'sensationalism_scoring_results.csv', index=False, encoding='utf-8'
    )


# ===================== 4. æ–°å¢ï¼šTopic-Level Sensationalism vs Fake News Correlationï¼ˆæé€Ÿç‰ˆï¼‰=====================
def topic_sensationalism_fake_news_correlation(
    fake_news_csv='fake_news.csv',  # å‡æ–°é—»CSVï¼ˆä»…å«titleåˆ—ï¼‰
    real_news_csv='real_news.csv',  # çœŸå®æ–°é—»CSVï¼ˆä»…å«titleåˆ—ï¼‰
    threshold=65,  # ç›¸ä¼¼åº¦é˜ˆå€¼ï¼ˆ0-100ï¼‰
    top_n=3  # æ¯ä¸ªæ ‡é¢˜åªåŒ¹é…å‰3ä¸ªæœ€ç›¸ä¼¼çš„æ ‡æ³¨ï¼ˆå‡å°‘è®¡ç®—é‡ï¼‰
):
    """
    åˆ†ææ¯ä¸ªä¸»é¢˜çš„è€¸äººå¬é—»ç¨‹åº¦ä¸å‡æ–°é—»æ¯”ä¾‹çš„ç›¸å…³æ€§ï¼ˆæé€Ÿç‰ˆï¼‰
    æ ¸å¿ƒä¼˜åŒ–ï¼šç”¨rapidfuzzæ›¿ä»£fuzzywuzzyï¼Œå€’æ’ç´¢å¼•ç¼©å°æ¯”å¯¹èŒƒå›´ï¼Œæ‰¹é‡åŒ¹é…
    """
    print("\n" + "=" * 70)
    print("=== ä¸»é¢˜çº§è€¸äººå¬é—»ç¨‹åº¦ vs å‡æ–°é—»æ¯”ä¾‹ ç›¸å…³æ€§åˆ†æï¼ˆæé€Ÿç‰ˆï¼‰===")
    print("=" * 70)

    # 1. åŠ è½½å¸¦è€¸äººå¬é—»åˆ†æ•°çš„æ•°æ®é›†ï¼ˆå¤ç”¨ä¹‹å‰çš„è®¡ç®—ç»“æœï¼‰
    try:
        df_sens = pd.read_pickle('full_data_with_sensationalism_score.pkl')
        print("âœ… æˆåŠŸåŠ è½½å¸¦è€¸äººå¬é—»åˆ†æ•°çš„æ•°æ®é›†")
    except FileNotFoundError:
        print("âš ï¸  æœªæ‰¾åˆ°è€¸äººå¬é—»åˆ†æ•°æ•°æ®ï¼Œæ­£åœ¨é‡æ–°è®¡ç®—...")
        df_sens = calculate_sensationalism_score()

    # å®šä¹‰æ ‡é¢˜æ¸…æ´—å‡½æ•°ï¼ˆç»Ÿä¸€é€»è¾‘ï¼Œä¿ç•™å…³é”®è¯ï¼‰
    def clean_title(title):
        if pd.isna(title):
            return ""
        # æ¸…æ´—ï¼šå»å‰åç©ºæ ¼ã€å°å†™ã€åªä¿ç•™å­—æ¯/æ•°å­—/ç©ºæ ¼ï¼ˆå»é™¤æ— æ„ä¹‰ç¬¦å·ï¼‰
        title = str(title).strip().lower()
        title = re.sub(r'[^\w\s]', '', title)  # åªä¿ç•™å•è¯å’Œç©ºæ ¼
        title = re.sub(r'\s+', ' ', title)     # åˆå¹¶å¤šä¸ªç©ºæ ¼ä¸ºä¸€ä¸ª
        return title

    # 2. åŠ è½½å¹¶å¤„ç†å‡æ–°é—»+çœŸå®æ–°é—»æ•°æ®
    def load_news_data(csv_path, is_fake_label):
        try:
            df = pd.read_csv(csv_path)
            title_cols = [col for col in df.columns if col.strip().lower() == 'title']
            if not title_cols:
                raise ValueError(f"CSVæ–‡ä»¶éœ€åŒ…å«'title'åˆ—ï¼Œå½“å‰åˆ—åï¼š{df.columns.tolist()}")
            df = df.rename(columns={title_cols[0]: 'headline_text'}).reset_index(drop=True)
            df['is_fake'] = is_fake_label
            df = df.drop_duplicates(subset=['headline_text']).reset_index(drop=True)
            df['headline_text_clean'] = df['headline_text'].apply(clean_title)
            # è¿‡æ»¤ç©ºæ ‡é¢˜ï¼ˆé¿å…æ— æ•ˆè®¡ç®—ï¼‰
            df = df[df['headline_text_clean'] != ""].reset_index(drop=True)
            print(f"âœ… åŠ è½½{'å‡æ–°é—»' if is_fake_label == 1 else 'çœŸå®æ–°é—»'}æ•°æ®æˆåŠŸï¼Œå…± {len(df)} æ¡æœ‰æ•ˆæ ‡é¢˜")
            return df[['headline_text', 'headline_text_clean', 'is_fake']]
        except Exception as e:
            raise Exception(f"{'å‡æ–°é—»' if is_fake_label == 1 else 'çœŸå®æ–°é—»'}åŠ è½½å¤±è´¥ï¼š{e}")

    try:
        fake_df = load_news_data(fake_news_csv, is_fake_label=1)
        real_df = load_news_data(real_news_csv, is_fake_label=0)
        fake_news_df = pd.concat([fake_df, real_df], ignore_index=True)
        print(f"âœ… åˆå¹¶åå…± {len(fake_news_df)} æ¡æœ‰æ•ˆæ ‡æ³¨ï¼ˆå‡æ–°é—»ï¼š{len(fake_df)} æ¡ï¼ŒçœŸå®æ–°é—»ï¼š{len(real_df)} æ¡ï¼‰")
    except Exception as e:
        print(f"âŒ æ ‡æ³¨æ•°æ®åŠ è½½å¤±è´¥ï¼š{e}")
        return

    # 3. ä¸»æ•°æ®é›†æ¸…æ´—+å»é‡
    df_sens['headline_text_clean'] = df_sens['headline_text'].apply(clean_title)
    df_sens = df_sens[df_sens['headline_text_clean'] != ""].reset_index(drop=True)
    df_sens = df_sens.drop_duplicates(subset=['headline_text_clean']).reset_index(drop=True)
    print(f"âœ… ä¸»æ•°æ®é›†æ¸…æ´—å®Œæˆï¼šå…± {len(df_sens)} æ¡å»é‡åæœ‰æ•ˆæ ‡é¢˜")

    # 4. å…³é”®æé€Ÿï¼šå®‰è£…å¹¶ä½¿ç”¨rapidfuzzï¼ˆæ¯”fuzzywuzzyå¿«10-100å€ï¼‰
    try:
        from rapidfuzz import process, fuzz
    except ImportError:
        print("âš ï¸  æœªå®‰è£…é«˜æ•ˆåŒ¹é…åº“ï¼Œæ­£åœ¨è‡ªåŠ¨å®‰è£…ï¼ˆä»…éœ€ä¸€æ¬¡ï¼‰...")
        import subprocess
        import sys
        subprocess.check_call([sys.executable, "-m", "pip", "install", "rapidfuzz"])
        from rapidfuzz import process, fuzz

    # 5. æ ¸å¿ƒä¼˜åŒ–ï¼šå€’æ’ç´¢å¼•ï¼ˆç¼©å°æ¯”å¯¹èŒƒå›´ï¼Œé¿å…å…¨é‡åŒ¹é…ï¼‰
    def build_inverted_index(text_list):
        """æ„å»ºå€’æ’ç´¢å¼•ï¼šå…³é”®è¯â†’åŒ…å«è¯¥å…³é”®è¯çš„æ–‡æœ¬ç´¢å¼•"""
        inverted_index = {}
        for idx, text in enumerate(text_list):
            words = text.split()  # æŒ‰ç©ºæ ¼åˆ†è¯
            for word in words:
                if len(word) < 2:  # è¿‡æ»¤å•å­—ç¬¦å…³é”®è¯ï¼ˆæ— æ„ä¹‰ï¼‰
                    continue
                if word not in inverted_index:
                    inverted_index[word] = set()
                inverted_index[word].add(idx)
        return inverted_index

    # ä¸ºæ ‡æ³¨é›†æ„å»ºå€’æ’ç´¢å¼•ï¼ˆåŸºäºæ¸…æ´—åçš„æ ‡é¢˜å…³é”®è¯ï¼‰
    target_texts = fake_news_df['headline_text_clean'].tolist()
    target_is_fake = fake_news_df['is_fake'].tolist()
    inverted_index = build_inverted_index(target_texts)
    print(f"âœ… å€’æ’ç´¢å¼•æ„å»ºå®Œæˆï¼šå…± {len(inverted_index)} ä¸ªå…³é”®è¯")

    # 6. æ‰¹é‡æ¨¡ç³ŠåŒ¹é…ï¼ˆåªåœ¨ç›¸å…³æ ‡æ³¨ä¸­æ¯”å¯¹ï¼Œå¤§å¹…æé€Ÿï¼‰
    def batch_fuzzy_match(main_texts, target_texts, target_is_fake, inverted_index, threshold=80, top_n=3):
        print(f"ğŸ” æ‰¹é‡æ¨¡ç³ŠåŒ¹é…ï¼ˆé˜ˆå€¼ï¼š{threshold}ï¼Œæ¯ä¸ªæ ‡é¢˜åŒ¹é…å‰{top_n}ä¸ªå€™é€‰ï¼‰...")
        match_is_fake = []
        batch_size = 1000  # åˆ†æ‰¹å¤„ç†ï¼Œé¿å…å†…å­˜å ç”¨è¿‡å¤§

        for i in range(0, len(main_texts), batch_size):
            batch_texts = main_texts[i:i+batch_size]
            # æ¯æ‰¹è¾“å‡ºè¿›åº¦
            print(f"ğŸ”„ å¤„ç†ç¬¬ {i//batch_size + 1} æ‰¹ï¼ˆå…± {len(main_texts)//batch_size + 1} æ‰¹ï¼‰...")

            for text in batch_texts:
                if not text:
                    match_is_fake.append(np.nan)
                    continue

                # æ­¥éª¤1ï¼šæå–å½“å‰æ ‡é¢˜çš„å…³é”®è¯ï¼Œæ‰¾åˆ°ç›¸å…³æ ‡æ³¨çš„ç´¢å¼•ï¼ˆç¼©å°æ¯”å¯¹èŒƒå›´ï¼‰
                words = text.split()
                related_indices = set()
                for word in words:
                    if len(word) < 2:
                        continue
                    if word in inverted_index:
                        related_indices.update(inverted_index[word])
                related_indices = list(related_indices)

                # æ­¥éª¤2ï¼šå¦‚æœæ— ç›¸å…³æ ‡æ³¨ï¼Œæ ‡è®°ä¸ºæœªåŒ¹é…
                if not related_indices:
                    match_is_fake.append(np.nan)
                    continue

                # æ­¥éª¤3ï¼šåªåœ¨ç›¸å…³æ ‡æ³¨ä¸­åŒ¹é…ï¼ˆæ ¸å¿ƒæé€Ÿç‚¹ï¼‰
                related_targets = [target_texts[idx] for idx in related_indices]
                related_is_fake = [target_is_fake[idx] for idx in related_indices]

                # æ­¥éª¤4ï¼šå¿«é€ŸåŒ¹é…ï¼ˆç”¨rapidfuzzçš„process.extractï¼Œæ¯”fuzzywuzzyå¿«10å€ï¼‰
                matches = process.extract(
                    text,
                    related_targets,
                    scorer=fuzz.token_sort_ratio,  # ä¿æŒåŸæœ‰çš„åŒ¹é…é€»è¾‘
                    limit=top_n,  # åªå–å‰Nä¸ªæœ€ç›¸ä¼¼çš„
                    score_cutoff=threshold  # ä½äºé˜ˆå€¼çš„ç›´æ¥è¿‡æ»¤
                )

                # æ­¥éª¤5ï¼šå–ç›¸ä¼¼åº¦æœ€é«˜çš„åŒ¹é…ç»“æœ
                if matches:
                    best_match = max(matches, key=lambda x: x[1])
                    best_idx = related_targets.index(best_match[0])
                    match_is_fake.append(related_is_fake[best_idx])
                else:
                    match_is_fake.append(np.nan)

        return match_is_fake

    # æ‰§è¡Œæ‰¹é‡åŒ¹é…
    main_texts = df_sens['headline_text_clean'].tolist()
    df_sens['is_fake'] = batch_fuzzy_match(
        main_texts, target_texts, target_is_fake, inverted_index,
        threshold=threshold, top_n=top_n
    )

    # è¿‡æ»¤æœªåŒ¹é…åˆ°çš„è®°å½•
    df_combined = df_sens[df_sens['is_fake'].notna()].reset_index(drop=True)
    print(f"âœ… åŒ¹é…å®Œæˆï¼šå…± {len(df_combined)} æ¡æœ‰æ•ˆåŒ¹é…è®°å½•")

    if len(df_combined) < 100:
        print("âš ï¸  åŒ¹é…ç»“æœè¿‡å°‘ï¼Œå»ºè®®é™ä½é˜ˆå€¼ï¼ˆå¦‚70ï¼‰æˆ–å‡å°‘å…³é”®è¯è¿‡æ»¤ï¼ˆå¦‚ä¿ç•™å•å­—ç¬¦å…³é”®è¯ï¼‰")
        return

    # 7. åç»­ä¸»é¢˜çº§æŒ‡æ ‡è®¡ç®—ã€ç›¸å…³æ€§åˆ†æã€å¯è§†åŒ–ï¼ˆé€»è¾‘ä¸å˜ï¼‰
    topic_metrics = df_combined.groupby('lda_topic').agg(
        ä¸»é¢˜æ–°é—»æ€»æ•°=('headline_text', 'count'),
        å‡æ–°é—»æ•°é‡=('is_fake', 'sum'),
        å¹³å‡è€¸äººå¬é—»åˆ†æ•°=('sensationalism_score', 'mean')
    ).reset_index()

    topic_metrics['å‡æ–°é—»æ¯”ä¾‹'] = topic_metrics['å‡æ–°é—»æ•°é‡'] / topic_metrics['ä¸»é¢˜æ–°é—»æ€»æ•°'].replace(0, 1)
    topic_metrics = topic_metrics[topic_metrics['ä¸»é¢˜æ–°é—»æ€»æ•°'] >= 20].reset_index(drop=True)

    if len(topic_metrics) < 3:
        print("âš ï¸  æœ‰æ•ˆä¸»é¢˜æ•°é‡è¿‡å°‘ï¼ˆ<3ï¼‰ï¼Œæ— æ³•è¿›è¡Œç›¸å…³æ€§åˆ†æ")
        return

    print(f"\nğŸ“Š ä¸»é¢˜çº§æŒ‡æ ‡ç»Ÿè®¡ï¼ˆè¿‡æ»¤åï¼‰ï¼š")
    print(topic_metrics[['lda_topic', 'ä¸»é¢˜æ–°é—»æ€»æ•°', 'å‡æ–°é—»æ¯”ä¾‹', 'å¹³å‡è€¸äººå¬é—»åˆ†æ•°']].round(3))

    # ç›¸å…³æ€§åˆ†æ
    x = topic_metrics['å¹³å‡è€¸äººå¬é—»åˆ†æ•°']
    y = topic_metrics['å‡æ–°é—»æ¯”ä¾‹']
    corr_coef, p_value = pearsonr(x, y)

    print(f"\nğŸ“ˆ ç›¸å…³æ€§åˆ†æç»“æœï¼š")
    print(f"çš®å°”é€Šç›¸å…³ç³»æ•°ï¼ˆrï¼‰ï¼š{corr_coef:.3f}")
    print(f"æ˜¾è‘—æ€§æ°´å¹³ï¼ˆpå€¼ï¼‰ï¼š{p_value:.3f}")
    if p_value < 0.05:
        significance = "æ˜¾è‘—ï¼ˆp<0.05ï¼‰"
        interpretation = f"æ­£ç›¸å…³ï¼šè€¸äººå¬é—»ç¨‹åº¦è¶Šé«˜çš„ä¸»é¢˜ï¼Œå‡æ–°é—»æ¯”ä¾‹è¶Šé«˜ï¼ˆr={corr_coef:.3f}ï¼‰" if corr_coef > 0 else f"è´Ÿç›¸å…³ï¼šè€¸äººå¬é—»ç¨‹åº¦è¶Šé«˜çš„ä¸»é¢˜ï¼Œå‡æ–°é—»æ¯”ä¾‹è¶Šä½ï¼ˆr={corr_coef:.3f}ï¼‰"
    else:
        significance = "ä¸æ˜¾è‘—ï¼ˆpâ‰¥0.05ï¼‰"
        interpretation = "æœªæ£€æµ‹åˆ°æ˜¾è‘—çš„çº¿æ€§ç›¸å…³å…³ç³»"
    print(f"ç»“æœè§£è¯»ï¼š{interpretation}ï¼ˆ{significance}ï¼‰")

    # å¯è§†åŒ–
    fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(18, 6))

    # æ•£ç‚¹å›¾+æ‹Ÿåˆçº¿
    ax1.scatter(x, y, color='darkred', alpha=0.7, s=60, label=f'r={corr_coef:.3f}, p={p_value:.3f}')
    z = np.polyfit(x, y, 1)
    p = np.poly1d(z)
    ax1.plot(x, p(x), "b--", alpha=0.8, linewidth=2)
    ax1.set_xlabel('Average Sensationalism Score', fontsize=10)
    ax1.set_ylabel('Fake News Ratio', fontsize=10)
    ax1.set_title('Sensationalism Score vs Fake News Ratio\n(Scatter Plot with Trend Line)', fontsize=11, fontweight='bold')
    ax1.legend()
    ax1.grid(True, alpha=0.3)

    # åŒæŒ‡æ ‡æ¡å½¢å›¾
    topic_ids = topic_metrics['lda_topic'].astype(str)
    x_pos = np.arange(len(topic_ids))
    width = 0.35
    ax2.bar(x_pos - width/2, topic_metrics['å¹³å‡è€¸äººå¬é—»åˆ†æ•°'], width, label='Avg Sensationalism Score', color='purple', alpha=0.7)
    ax2.set_xlabel('Topic ID', fontsize=10)
    ax2.set_ylabel('Avg Sensationalism Score', fontsize=10, color='purple')
    ax2.tick_params(axis='y', labelcolor='purple')
    ax2.set_xticks(x_pos)
    ax2.set_xticklabels(topic_ids)
    ax2_twin = ax2.twinx()
    ax2_twin.bar(x_pos + width/2, topic_metrics['å‡æ–°é—»æ¯”ä¾‹'], width, label='Fake News Ratio', color='orange', alpha=0.7)
    ax2_twin.set_ylabel('Fake News Ratio', fontsize=10, color='orange')
    ax2_twin.tick_params(axis='y', labelcolor='orange')
    lines1, labels1 = ax2.get_legend_handles_labels()
    lines2, labels2 = ax2_twin.get_legend_handles_labels()
    ax2.legend(lines1 + lines2, labels1 + labels2, loc='upper right')
    ax2.set_title('Sensationalism Score & Fake News Ratio by Topic', fontsize=11, fontweight='bold')

    # çƒ­åŠ›å›¾
    heatmap_data = topic_metrics[['å¹³å‡è€¸äººå¬é—»åˆ†æ•°', 'å‡æ–°é—»æ¯”ä¾‹', 'ä¸»é¢˜æ–°é—»æ€»æ•°']].corr()
    sb.heatmap(heatmap_data, annot=True, fmt='.3f', cmap='RdBu_r', center=0, square=True, linewidths=0.5, ax=ax3)
    ax3.set_title('Correlation Heatmap of Topic-Level Metrics', fontsize=11, fontweight='bold')

    plt.tight_layout()
    plt.savefig('sensationalism_fake_news_correlation.png', dpi=150, bbox_inches='tight')
    print("\nğŸ“Š å¯è§†åŒ–å·²ä¿å­˜ï¼šsensationalism_fake_news_correlation.png")

    # ä¿å­˜ç»“æœ
    topic_metrics.to_csv('topic_sensationalism_fake_news_metrics.csv', index=False, encoding='utf-8')
    print("\nğŸ“„ è¯¦ç»†æŒ‡æ ‡å·²ä¿å­˜ï¼štopic_sensationalism_fake_news_metrics.csv")
# ===================== æ‰§è¡Œå…¥å£ =====================
if __name__ == "__main__":
    # 1. ç”Ÿæˆæ ‡æ³¨æ¨¡æ¿ï¼ˆå·²å®Œæˆæ ‡æ³¨ï¼Œæ³¨é‡Šæ‰ï¼‰
    # generate_annotation_template()

    # 2. è¿è¡Œ VADER vs RoBERTa æ¨¡å‹å¯¹æ¯”ï¼ˆæ ¸å¿ƒæ­¥éª¤ï¼‰
    #compare_vader_roberta(annotated_csv='sentiment_annotation_template.csv')  # æ ‡æ³¨æ–‡ä»¶è·¯å¾„ç¡®ä¿æ­£ç¡®

    # 3. å¼‚å¸¸æ£€æµ‹ï¼ˆç›´æ¥è¿è¡Œï¼‰
    #topic_sentiment_anomaly_detection()

    # 4. è€¸äººå¬é—»è¯„åˆ†ï¼ˆç›´æ¥è¿è¡Œï¼‰
    calculate_sensationalism_score()
    # 5. æ–°å¢ï¼šä¸»é¢˜çº§è€¸äººå¬é—»ç¨‹åº¦ vs å‡æ–°é—»æ¯”ä¾‹ ç›¸å…³æ€§åˆ†æï¼ˆå…³é”®æ–°å¢ï¼‰
    #topic_sensationalism_fake_news_correlation(
    #    fake_news_csv='Fake.csv',  # ä½ çš„å‡æ–°é—»CSVæ–‡ä»¶å
    #    real_news_csv='True.csv'  # ä½ çš„çœŸå®æ–°é—»CSVæ–‡ä»¶å
    #)

    print("\nğŸ‰ æ‰€æœ‰é«˜çº§åˆ†æå®Œæˆï¼")